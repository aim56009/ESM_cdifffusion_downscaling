{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0729e814-7b68-4004-a0c7-ed0c18b814f0",
   "metadata": {},
   "source": [
    "# SETUP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0833e901-84ea-42a9-a9ef-63d4977a2b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install xarray\n",
    "!pip install wandb\n",
    "!pip install collections\n",
    "!pip install pysteps\n",
    "!pip install beartype\n",
    "!pip install scikit-image\n",
    "!pip install netcdf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "989fd62e-12ab-493d-998e-dc2bbfb69766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/dss/dsshome1/0D/ge74xuf2/climate_diffusion'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "new_directory = '/dss/dsshome1/0D/ge74xuf2/climate_diffusion'\n",
    "os.chdir(new_directory)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df397714-5d3d-4701-a9dd-954e8439df60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pysteps configuration file found at: /usr/local/lib/python3.10/dist-packages/pysteps/pystepsrc\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import wandb\n",
    "import IPython.display as display\n",
    "import logging\n",
    "import torch.nn.functional as F\n",
    "import collections\n",
    "import copy\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import random_split\n",
    "from inspect import isfunction\n",
    "from functools import partial\n",
    "from abc import abstractmethod\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from pysteps.utils.spectral import rapsd, corrcoef\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from src.utils import *\n",
    "from src.utils_essential import *\n",
    "from src.base_network import BaseNetwork\n",
    "from src.imagen_unet import *\n",
    "from src.helper import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from src.dataloader_sr import gfdl_eval_256, era5_upscaled_1d_256, era5_0_25d_256, qm_gfdl_trafo_units_hr\n",
    "from src.dataloader_sr import QM_GFDL_LR_Dataset_256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9b50ef2-1799-4de4-b160-45f72e099369",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmichi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/dss/dsshome1/0D/ge74xuf2/climate_diffusion/wandb/run-20241016_121603-uer5h473</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/michi/climate-diffusion/runs/uer5h473' target=\"_blank\">zany-wood-2048</a></strong> to <a href='https://wandb.ai/michi/climate-diffusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/michi/climate-diffusion' target=\"_blank\">https://wandb.ai/michi/climate-diffusion</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/michi/climate-diffusion/runs/uer5h473' target=\"_blank\">https://wandb.ai/michi/climate-diffusion/runs/uer5h473</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {\"run_name\": \"revision_100\",     \n",
    "          \"epochs\":        400,\n",
    "          \"batch_size\":    2, \n",
    "          \"lr\":            1e-5, \n",
    "          \"image_size\":    256,             \n",
    "          \"device\":        \"cuda\", \n",
    "          \"num_workers\":   8, \n",
    "}\n",
    "\n",
    "wandb.init(project='climate-diffusion', entity='Michi',config=config, save_code=True)\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", level=logging.INFO, datefmt=\"%I:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9688cda-1816-42c6-9a10-85a76f308a5c",
   "metadata": {},
   "source": [
    "# Dataloaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bbb2d39-2a09-4e21-bfc2-070f32b5bce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR ERA5 torch.Size([2, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "era5_hr_ds = era5_0_25d_256(stage='train')\n",
    "era5_hr_dl = data.DataLoader(era5_hr_ds, batch_size=wandb.config.batch_size, shuffle=False, drop_last=True,\n",
    "                                     num_workers=wandb.config.num_workers)\n",
    "sample_era5_025_tr = next(iter(era5_hr_dl))\n",
    "print(\"HR ERA5\", sample_era5_025_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d431ee1-afb2-4c20-9e92-36f87efbd0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR ERA5 torch.Size([2, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "era5_lr_ds = era5_upscaled_1d_256(stage='train')\n",
    "era5_lr_dl = data.DataLoader(era5_lr_ds, batch_size=wandb.config.batch_size, shuffle=False, drop_last=True,\n",
    "                                     num_workers=wandb.config.num_workers)\n",
    "\n",
    "sample_era5_1d_256p = next(iter(era5_lr_dl))\n",
    "print(\"LR ERA5\", sample_era5_1d_256p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01968a8e-713d-4a9e-90d2-b68b0dd673de",
   "metadata": {},
   "source": [
    "## validation DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e362c83f-6be8-47f8-be18-728796cdd7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_valid = 1\n",
    "\n",
    "eval_true = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dab8f05-3255-4003-8256-64f5c40df43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "if eval_true == True:\n",
    "    era5_p_1d_256_v = era5_upscaled_1d_256(stage='valid')\n",
    "\n",
    "    dataloader_era5_val_1d_256 = data.DataLoader(era5_p_1d_256_v, batch_size=bs_valid, shuffle=False, drop_last=True,\n",
    "                                         num_workers=wandb.config.num_workers)\n",
    "\n",
    "    era5_lr = next(iter(dataloader_era5_val_1d_256))\n",
    "    print(era5_lr.shape)\n",
    "\n",
    "if eval_true == True:\n",
    "    era5_p025 = era5_0_25d_256(stage='valid')\n",
    "    dataloader_era5_val_p025 = data.DataLoader(era5_p025, batch_size=bs_valid, shuffle=False, drop_last=True,\n",
    "                                         num_workers=wandb.config.num_workers)\n",
    "\n",
    "    era5_hr = next(iter(dataloader_era5_val_p025))\n",
    "    print(era5_hr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17001583-d679-4700-99a4-42804b5eaa2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/0D/ge74xuf2/climate_diffusion/src/dataloader_sr.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.era5_qm = torch.load(self.path).cpu().numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets size torch.Size([1400, 64, 64])\n",
      "QM+US - GFDL LR 256 shape: torch.Size([1, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "#  qm-us inference QM-corrected gfdl\n",
    "if eval_true == True:\n",
    "\n",
    "    bc_gfdl_dataset_val = QM_GFDL_LR_Dataset_256('data/11_01_deltaQM_debiased_gfdl_valid_custom_dl.pth')\n",
    "\n",
    "    dataloader_bc_gfdl_val = data.DataLoader(bc_gfdl_dataset_val, batch_size=bs_valid, shuffle=False,\n",
    "                                      drop_last=True,num_workers=2)\n",
    "\n",
    "\n",
    "    bc_gfld_sample = next(iter(dataloader_bc_gfdl_val))\n",
    "    print(\"QM+US - GFDL LR 256 shape:\",bc_gfld_sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c308c52-8da4-441b-973e-3d2372a085da",
   "metadata": {},
   "source": [
    "# Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f56c4f1f-704a-4063-9c2c-cc3c29932ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDiffusionContinuousTimes(BaseNetwork):\n",
    "    def __init__(self, *, noise_schedule, timesteps = 1000,**kwargs):\n",
    "        super(GaussianDiffusionContinuousTimes, self).__init__(**kwargs)\n",
    "        \n",
    "        if noise_schedule == \"linear\":\n",
    "            self.log_snr = beta_linear_log_snr\n",
    "        elif noise_schedule == \"cosine\":\n",
    "            self.log_snr = alpha_cosine_log_snr\n",
    "        else:\n",
    "            raise ValueError(f'invalid noise schedule {noise_schedule}')\n",
    "\n",
    "        self.num_timesteps = timesteps\n",
    "\n",
    "    def get_times(self, batch_size, noise_level, *, device):\n",
    "        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n",
    "\n",
    "    def sample_random_times(self, batch_size, *, device):\n",
    "        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n",
    "\n",
    "    def get_condition(self, times):\n",
    "        return maybe(self.log_snr)(times)\n",
    "\n",
    "    def get_sampling_timesteps(self, batch, *, device):\n",
    "        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n",
    "        times = repeat(times, 't -> b t', b = batch)\n",
    "        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n",
    "        times = times.unbind(dim = -1)\n",
    "        return times\n",
    "\n",
    "    def q_posterior(self, x_start, x_t, t, *, t_next = None):\n",
    "        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n",
    "\n",
    "        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n",
    "        log_snr = self.log_snr(t)\n",
    "        log_snr_next = self.log_snr(t_next)\n",
    "        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n",
    "\n",
    "        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n",
    "        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n",
    "\n",
    "        # c - as defined near eq 33\n",
    "        c = -expm1(log_snr - log_snr_next)\n",
    "        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n",
    "        \n",
    "        '''\n",
    "        alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n",
    "        alpha_next * ( (x_t - x_t*c) / alpha + c * x_start)\n",
    "        alpha_next * ( x_t/ alpha - c*x_t/ alpha + c * x_start)\n",
    "        x_t*alpha_next/ alpha - c*x_t*alpha_next/ alpha + c * alpha_next*x_start)\n",
    "        alpha_next/ alpha ( x_t - c * x_t + c * alpha * x_start)      witht  x_start = (x_t -simga*noise)/alpha\n",
    "        alpha_next/alpha (x_t - c* x_t + c * alpha *[(x_t-simga*noise)/alpha])\n",
    "        alpha_next/alpha (x_t - c* x_t + c*x_t - c*simga*noise)\n",
    "        alpha_next/alpha (x_t - c*simga*noise)\n",
    "        alpha_next/alpha (x_t + c) (- alpha/alpha_next * sigma * noise)\n",
    "        '''\n",
    "        \n",
    "        # following (eq. 33)\n",
    "        posterior_variance = (sigma_next ** 2) * c\n",
    "        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    def q_sample(self, x_start, t, noise = None):\n",
    "        dtype = x_start.dtype\n",
    "\n",
    "        if isinstance(t, float):\n",
    "            batch = x_start.shape[0]\n",
    "            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n",
    "\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "        log_snr = self.log_snr(t).type(dtype)\n",
    "        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n",
    "        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n",
    "\n",
    "        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n",
    "\n",
    "    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n",
    "        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n",
    "        batch = shape[0]\n",
    "\n",
    "        if isinstance(from_t, float):\n",
    "            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n",
    "\n",
    "        if isinstance(to_t, float):\n",
    "            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n",
    "\n",
    "        noise = default(noise, lambda: torch.randn_like(x_from))\n",
    "\n",
    "        log_snr = self.log_snr(from_t)\n",
    "        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n",
    "        alpha, sigma = log_snr_to_alpha_sigma(log_snr_padded_dim)\n",
    "\n",
    "        log_snr_to = self.log_snr(to_t)\n",
    "        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n",
    "        alpha_to, sigma_to = log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n",
    "\n",
    "        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n",
    "\n",
    "    def predict_start_from_v(self, x_t, t, v):\n",
    "        log_snr = self.log_snr(t)\n",
    "        log_snr = right_pad_dims_to(x_t, log_snr)\n",
    "        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n",
    "        return alpha * x_t - sigma * v\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        log_snr = self.log_snr(t)\n",
    "        log_snr = right_pad_dims_to(x_t, log_snr)\n",
    "        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n",
    "        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea0524b-2776-4794-8f2c-fb27adb9c4ef",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa3f082a-d815-48b2-99ba-7a5b419313c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA():\n",
    "    def __init__(self, beta=0.9999):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "    def update_model_average(self, ma_model, current_model):\n",
    "        for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n",
    "            old_weight, up_weight = ma_params.data, current_params.data\n",
    "            ma_params.data = self.update_average(old_weight, up_weight)\n",
    "    def update_average(self, old, new):\n",
    "        if old is None:\n",
    "            return new\n",
    "        return old * self.beta + (1 - self.beta) * new\n",
    "\n",
    "\n",
    "class BaseModel():\n",
    "    def __init__(self, phase,  dataloader, metrics, n_epochs=10, batch_size = 8, \n",
    "                  save_checkpoint_epoch=10,resume_state=False,\n",
    "                 save_path_base=\"/dss/dssfs04/lwp-dss-0002/pn49fu/pn49fu-dss-0002/ge74xuf2/diffusion\", **kwargs):\n",
    "        \"\"\" init model with basic input, which are from __init__(**kwargs) function in inherited class \"\"\"\n",
    "        self.phase = phase\n",
    "        self.device = config['device']\n",
    "        \n",
    "        self.n_epochs = n_epochs\n",
    "        self.resume_state = resume_state\n",
    "        self.save_checkpoint_epoch = save_checkpoint_epoch\n",
    "\n",
    "        ''' optimizers and schedulers '''\n",
    "        self.schedulers = []\n",
    "        self.optimizers = []\n",
    "\n",
    "        ''' process record '''\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch = 0\n",
    "        self.iter = 0 \n",
    "\n",
    "        self.phase_loader = dataloader\n",
    "        self.metrics = metrics\n",
    "\n",
    "        ''' logger to log file, which only work on GPU 0. writer to tensorboard and result file '''\n",
    "        self.results_dict = CustomResult([],[])\n",
    "        \n",
    "        self.save_path_base = save_path_base\n",
    "\n",
    "    \n",
    "   \n",
    "    def train(self):\n",
    "        while self.epoch <= self.n_epochs: \n",
    "            self.epoch += 1\n",
    "            \n",
    "            train_log, condition_after_unet, original_img = self.train_step()\n",
    "            \n",
    "            print(\"epoch:\", self.epoch)\n",
    "            \n",
    "            if self.epoch % 1 == 0:\n",
    "                output_sampled = imagen.restoration(start_image_or_video=condition_after_unet.to(config[\"device\"]))\n",
    "                \n",
    "                output_sampled_test = wandb.Image(output_sampled)\n",
    "                wandb.log({\"diffusion gen img\": output_sampled_test})\n",
    "                condition_after_unet_wb = wandb.Image(condition_after_unet)\n",
    "                wandb.log({\"condition img\": condition_after_unet_wb})\n",
    "                original_img_wb = wandb.Image(original_img)\n",
    "                wandb.log({\"original img\": original_img_wb})\n",
    "                \n",
    "                print(\"proxy-ERA5\")\n",
    "                plot_images_no_lab(condition_after_unet)\n",
    "                print(\"SR proxy-ERA5\")\n",
    "                plot_images_no_lab(output_sampled)\n",
    "                print(\"ERA5\")\n",
    "                plot_images_no_lab(original_img)\n",
    "                \n",
    "                \n",
    "                print(\"BLUE: ORIGINAL DATA VS ORANGE: DIFFUSION MODEL GENERATED\")\n",
    "                latitudinal_mean_three(original=original_img, generated=output_sampled, \n",
    "                                       label=condition_after_unet.detach() , var=\"p\")\n",
    "                print(\"BLUE: ORIGINAL DATA VS ORANGE: DIFFUSION MODEL GENERATED, GREEN: UNET GENERATED\")\n",
    "                histograms_three(original=original_img.detach(), generated=output_sampled.detach(),\n",
    "                                 label= condition_after_unet.detach(),xlim_end=None, var=\"p\")\n",
    "                \n",
    "                \n",
    "                ssd = SpatialSpectralDensity_diff_res( \n",
    "                                      original_img.detach().cpu().numpy()\n",
    "                                     ,output_sampled.detach().cpu().numpy()\n",
    "                                     ,condition_after_unet.detach().cpu().numpy()\n",
    "                                     ,new_labels = [\"era5 hr\",\" sr era5\",\"era5 lr\"])\n",
    "                ssd.run(num_times=None)\n",
    "                ssd.plot_psd(fname=\"\",model_resolution=0.25,model_resolution_2=0.25)\n",
    "                \n",
    "                plt.show()\n",
    "                  \n",
    "                    \n",
    "            if self.epoch % self.save_checkpoint_epoch == 0:\n",
    "                print('Saving the self at the end of epoch {:.0f}'.format(self.epoch))\n",
    "                self.save_everything()\n",
    "\n",
    "    def test(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_step(self):\n",
    "        raise NotImplementedError('You must specify how to train your networks.')\n",
    "\n",
    "\n",
    "    def test_step(self):\n",
    "        pass\n",
    "    \n",
    "    def print_network(self, network):\n",
    "        \"\"\" print network structure, only work on GPU 0 \"\"\"\n",
    "        if isinstance(network, nn.DataParallel) or isinstance(network, nn.parallel.DistributedDataParallel):\n",
    "            network = network.module\n",
    "        \n",
    "        s, n = str(network), sum(map(lambda x: x.numel(), network.parameters()))\n",
    "        net_struc_str = '{}'.format(network.__class__.__name__)\n",
    "        print('Network structure: {}, with parameters: {:,d}'.format(net_struc_str, n))\n",
    "        print(s)\n",
    "\n",
    "    def save_network(self, network, network_label):\n",
    "        \"\"\" save network structure, only work on GPU 0 \"\"\"\n",
    "        save_filename = '{}_{}.pth'.format(self.epoch, network_label)\n",
    "        #save_path = os.path.join(os.path.join(\"models\", config['run_name'], save_filename))\n",
    "        save_path = os.path.join(self.save_path_base,  config['run_name'],save_filename)\n",
    "        \n",
    "        if isinstance(network, nn.DataParallel) or isinstance(network, nn.parallel.DistributedDataParallel):\n",
    "            network = network.module\n",
    "        state_dict = network.state_dict()\n",
    "        for key, param in state_dict.items():\n",
    "            state_dict[key] = param.cpu()\n",
    "        torch.save(state_dict, save_path)\n",
    "\n",
    "    def load_network(self, network, network_label, model_path, strict=True):        \n",
    "        if not os.path.exists(model_path):\n",
    "            print('Pretrained model in [{:s}] is not existed, Skip it'.format(model_path))\n",
    "            return\n",
    "\n",
    "        print('Loading pretrained model from [{:s}] ...'.format(model_path))\n",
    "        network.load_state_dict(torch.load(model_path), strict=strict)\n",
    "        network.to(self.device)\n",
    "\n",
    "    def save_training_state(self):\n",
    "        \"\"\" saves training state during training, only work on GPU 0 \"\"\"\n",
    "\n",
    "        assert isinstance(self.optimizers, list) and isinstance(self.schedulers, list), 'optimizers and schedulers must be a list.'\n",
    "        state = {'epoch': self.epoch, 'iter': self.iter, 'schedulers': [], 'optimizers': []}\n",
    "        for s in self.schedulers:\n",
    "            state['schedulers'].append(s.state_dict())\n",
    "        for o in self.optimizers:\n",
    "            state['optimizers'].append(o.state_dict())\n",
    "        save_filename = '{}.state'.format(self.epoch)\n",
    "        save_path = os.path.join(os.path.join(self.save_path_base, config['run_name'], save_filename))\n",
    "        torch.save(state, save_path)\n",
    "\n",
    "    def resume_training(self):\n",
    "        \"\"\" resume the optimizers and schedulers for training, only work when phase is test or resume training enable \"\"\"\n",
    "        if self.phase!='train' or self.resume_state is None:\n",
    "            return\n",
    "        print('Beign loading training states'.format())\n",
    "        assert isinstance(self.optimizers, list) and isinstance(self.schedulers, list), 'optimizers and schedulers must be a list.'\n",
    "        \n",
    "        state_path = \"{}.state\".format(self.resume_state)\n",
    "        \n",
    "        if not os.path.exists(state_path):\n",
    "            print('Training state in [{:s}] is not existed, Skip it'.format(state_path))\n",
    "            return\n",
    "\n",
    "        print('Loading training state for [{:s}] ...'.format(state_path))\n",
    "        resume_state = torch.load(state_path)#.to(self.device) \n",
    "        \n",
    "        resume_optimizers = resume_state['optimizers']\n",
    "        resume_schedulers = resume_state['schedulers']\n",
    "        assert len(resume_optimizers) == len(self.optimizers), 'Wrong lengths of optimizers {} != {}'.format(len(resume_optimizers), len(self.optimizers))\n",
    "        assert len(resume_schedulers) == len(self.schedulers), 'Wrong lengths of schedulers {} != {}'.format(len(resume_schedulers), len(self.schedulers))\n",
    "        for i, o in enumerate(resume_optimizers):\n",
    "            self.optimizers[i].load_state_dict(o)\n",
    "        for i, s in enumerate(resume_schedulers):\n",
    "            self.schedulers[i].load_state_dict(s)\n",
    "\n",
    "        self.epoch = resume_state['epoch']\n",
    "        self.iter = resume_state['iter']\n",
    "\n",
    "    def load_everything(self):\n",
    "        pass \n",
    "    \n",
    "    @abstractmethod\n",
    "    def save_everything(self):\n",
    "        raise NotImplementedError('You must specify how to save your networks, optimizers and schedulers.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d0ff009-dab1-46f4-9220-a50579025cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Imagen(BaseNetwork): \n",
    "    def __init__(\n",
    "        self,\n",
    "        unets,\n",
    "        *,\n",
    "        image_sizes,                                # for cascading ddpm, image size at each stage\n",
    "        channels = 1,\n",
    "        timesteps = 1000,\n",
    "        cond_drop_prob = 0.1,\n",
    "        loss_type = 'l2',\n",
    "        noise_schedules = 'cosine',\n",
    "        pred_objectives = 'v',\n",
    "        lowres_noise_schedule = 'linear',\n",
    "        lowres_sample_noise_level = 0.2,            # in the paper, they present a new trick where they noise the lowres conditioning image, and at sample time, fix it to a certain level (0.1 or 0.3) - the unets are also made to be conditioned on this noise level\n",
    "        dynamic_thresholding = True,\n",
    "        dynamic_thresholding_percentile = 0.95,     # unsure what this was based on perusal of paper\n",
    "        resize_mode = 'nearest',\n",
    "        min_snr_loss_weight = True,                 # https://arxiv.org/abs/2303.09556\n",
    "        min_snr_gamma = 5\n",
    "        ,**kwargs):\n",
    "\n",
    "        #super(Imagen).__init__( **kwargs)\n",
    "        super(Imagen, self).__init__(**kwargs)\n",
    "        \n",
    "\n",
    "        # loss \n",
    "        self.loss_type = loss_type\n",
    "        self.loss_fn = F.mse_loss\n",
    "\n",
    "        # channels\n",
    "        self.channels = channels\n",
    "        self.noise_schedulers = GaussianDiffusionContinuousTimes(noise_schedule = noise_schedules, timesteps=timesteps)\n",
    "        print(\"timesteps\",self.noise_schedulers.num_timesteps)\n",
    "        \n",
    "        # lowres augmentation noise schedule\n",
    "        self.lowres_noise_schedule = GaussianDiffusionContinuousTimes(noise_schedule = lowres_noise_schedule )\n",
    "\n",
    "        # ddpm objectives - predicting noise by default\n",
    "        self.pred_objectives = pred_objectives\n",
    "\n",
    "\n",
    "        # construct unets\n",
    "\n",
    "        self.unets = unets.cast_model_parameters(lowres_cond = True,\n",
    "                                                channels = self.channels,\n",
    "                                                channels_out = 1)\n",
    "            \n",
    "        # unet image sizes\n",
    "\n",
    "        self.image_sizes = image_sizes\n",
    "        self.sample_channels = self.channels\n",
    "        self.resize_to = resize_image_to\n",
    "\n",
    "\n",
    "        # cascading ddpm related stuff\n",
    "\n",
    "        lowres_conditions = lambda t: t.lowres_cond\n",
    "        self.lowres_sample_noise_level = lowres_sample_noise_level\n",
    "\n",
    "        # classifier free guidance\n",
    "\n",
    "        self.cond_drop_prob = cond_drop_prob\n",
    "        self.can_classifier_guidance = cond_drop_prob > 0.\n",
    "\n",
    "        # dynamic thresholding\n",
    "\n",
    "        self.dynamic_thresholding = dynamic_thresholding\n",
    "        self.dynamic_thresholding_percentile = dynamic_thresholding_percentile\n",
    "\n",
    "        # min snr loss weight\n",
    "\n",
    "        min_snr_loss_weight = min_snr_loss_weight\n",
    "        min_snr_gamma = min_snr_gamma\n",
    "\n",
    "        self.min_snr_gamma = min_snr_gamma if min_snr_loss_weight else None\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def device(self):\n",
    "        return self._temp.device\n",
    "\n",
    "\n",
    "    def reset_unets_all_one_device(self, device = None):\n",
    "        device = default(device, \"cuda\")\n",
    "        self.unets.to(device)\n",
    "\n",
    "        self.unet_being_trained_index = -1\n",
    "\n",
    "    @contextmanager\n",
    "    def one_unet_in_gpu(self,  unet = None):\n",
    "        cpu = torch.device('cpu')\n",
    "        devices = module_device(unet) \n",
    "        self.unets.to(cpu)\n",
    "        unet.to(\"cuda\")\n",
    "        yield\n",
    "        unet.to(devices)\n",
    "\n",
    "    # gaussian diffusion methods\n",
    "    def p_mean_variance(\n",
    "        self,\n",
    "        unet,\n",
    "        x,\n",
    "        t,\n",
    "        *,\n",
    "        noise_scheduler,\n",
    "        lowres_cond_img = None,\n",
    "        lowres_noise_times = None,\n",
    "        cond_scale = 1.,\n",
    "        model_output = None,\n",
    "        t_next = None,\n",
    "        pred_objective = 'noise',\n",
    "        dynamic_threshold = True):\n",
    "        \n",
    "        assert not (cond_scale != 1. and not self.can_classifier_guidance), 'imagen was not trained with conditional dropout, and thus one cannot use classifier free guidance (cond_scale anything other than 1)'\n",
    "\n",
    "\n",
    "        pred = default(model_output, lambda: unet.forward(\n",
    "            x,\n",
    "            noise_scheduler.get_condition(t),\n",
    "            #cond_scale = cond_scale,                # for classifier free guidance\n",
    "            lowres_cond_img = lowres_cond_img,\n",
    "            lowres_noise_times = self.lowres_noise_schedule.get_condition(lowres_noise_times) ))\n",
    "\n",
    "        if pred_objective == 'noise':\n",
    "            x_start = noise_scheduler.predict_start_from_noise(x, t = t, noise = pred)\n",
    "        elif pred_objective == 'x_start':\n",
    "            x_start = pred\n",
    "        elif pred_objective == 'v':\n",
    "            x_start = noise_scheduler.predict_start_from_v(x, t = t, v = pred)\n",
    "        else:\n",
    "            raise ValueError(f'unknown objective {pred_objective}')\n",
    "\n",
    "        if dynamic_threshold:\n",
    "            s = torch.quantile(\n",
    "                rearrange(x_start, 'b ... -> b (...)').abs(),\n",
    "                self.dynamic_thresholding_percentile,\n",
    "                dim = -1\n",
    "            )\n",
    "\n",
    "            s.clamp_(min = 1.)\n",
    "            s = right_pad_dims_to(x_start, s)\n",
    "            x_start = x_start.clamp(-s, s) / s\n",
    "        else:\n",
    "            x_start.clamp_(-1., 1.)\n",
    "\n",
    "        mean_and_variance = noise_scheduler.q_posterior(x_start = x_start, x_t = x, t = t, t_next = t_next)\n",
    "        return mean_and_variance, x_start\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(\n",
    "        self,\n",
    "        unet,\n",
    "        x,\n",
    "        t,\n",
    "        *,\n",
    "        noise_scheduler,\n",
    "        t_next = None,\n",
    "        cond_scale = 1.,\n",
    "        lowres_cond_img = None,\n",
    "        lowres_noise_times = None,\n",
    "        pred_objective = 'noise',\n",
    "        dynamic_threshold = True\n",
    "    ):\n",
    "        b, *_, device = *x.shape, x.device\n",
    "\n",
    "\n",
    "        (model_mean, _, model_log_variance), x_start = self.p_mean_variance(\n",
    "            unet,\n",
    "            x = x,\n",
    "            t = t,\n",
    "            t_next = t_next,\n",
    "            noise_scheduler = noise_scheduler,\n",
    "            cond_scale = cond_scale,\n",
    "            lowres_cond_img = lowres_cond_img,\n",
    "            lowres_noise_times = lowres_noise_times,\n",
    "            pred_objective = pred_objective,\n",
    "            dynamic_threshold = dynamic_threshold,\n",
    "        )\n",
    "\n",
    "        noise = torch.randn_like(x)\n",
    "        # no noise when t == 0\n",
    "        is_last_sampling_timestep = (t_next == 0) if isinstance(noise_scheduler, GaussianDiffusionContinuousTimes) else (t == 0)\n",
    "        nonzero_mask = (1 - is_last_sampling_timestep.float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n",
    "        pred = model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n",
    "        return pred, x_start\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(\n",
    "        self,\n",
    "        unet,\n",
    "        shape,\n",
    "        *,\n",
    "        noise_scheduler,\n",
    "        lowres_cond_img = None,\n",
    "        lowres_noise_times = None,\n",
    "        cond_scale = 1,\n",
    "        pred_objective = 'noise',\n",
    "        dynamic_threshold = True,\n",
    "        use_tqdm = True\n",
    "        ):\n",
    "        \n",
    "        device =\"cuda\"\n",
    "        \n",
    "        batch = shape[0]\n",
    "        img = torch.randn(shape, device = device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x_start = None\n",
    "\n",
    "        # time\n",
    "        timesteps = noise_scheduler.get_sampling_timesteps(batch, device = device)\n",
    "\n",
    "\n",
    "        timesteps = timesteps[0:]\n",
    "\n",
    "        for times, times_next in tqdm(timesteps, desc = 'sampling loop time step', total = len(timesteps), disable = not use_tqdm):\n",
    "            is_last_timestep = times_next == 0\n",
    "\n",
    "            for r in reversed(range(1)):\n",
    "                is_last_resample_step = r == 0\n",
    "\n",
    "                img, x_start = self.p_sample(\n",
    "                    unet,\n",
    "                    img,\n",
    "                    times,\n",
    "                    t_next = times_next,\n",
    "                    cond_scale = cond_scale,\n",
    "                    lowres_cond_img = lowres_cond_img,\n",
    "                    lowres_noise_times = lowres_noise_times,\n",
    "                    noise_scheduler = noise_scheduler,\n",
    "                    pred_objective = pred_objective,\n",
    "                    dynamic_threshold = dynamic_threshold,\n",
    "                )\n",
    "                \n",
    "        \n",
    "        img.clamp_(-1., 1.)\n",
    "        \n",
    "        return img\n",
    "\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    @eval_decorator\n",
    "    @beartype\n",
    "    def restoration(\n",
    "        self,\n",
    "        batch_size = 1,\n",
    "        cond_scale = 1.,\n",
    "        lowres_sample_noise_level = None,\n",
    "        start_image_or_video = None,\n",
    "        device = None,\n",
    "        use_tqdm = True,\n",
    "        use_one_unet_in_gpu = True\n",
    "        ):\n",
    "\n",
    "        device = default(device, \"cuda\")\n",
    "        self.reset_unets_all_one_device(device = device)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        is_cuda = next(self.parameters()).is_cuda\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        lowres_sample_noise_level = default(lowres_sample_noise_level, self.lowres_sample_noise_level)\n",
    "\n",
    "\n",
    "        # condition scaling\n",
    "        cond_scale = cond_scale\n",
    "\n",
    "\n",
    "        assert exists(start_image_or_video), 'starting image must be supplied if only doing upscaling'\n",
    "\n",
    "        prev_image_size = self.image_sizes\n",
    "        prev_frame_size =  None\n",
    "        \n",
    "        \n",
    "        img = self.resize_to(start_image_or_video, prev_image_size)\n",
    "        \n",
    "\n",
    "        unet = self.unets\n",
    "        channel = self.sample_channels\n",
    "        image_size = self.image_sizes\n",
    "        noise_scheduler = self.noise_schedulers\n",
    "        pred_objective = self.pred_objectives\n",
    "        dynamic_threshold = self.dynamic_thresholding\n",
    "        unet_cond_scale = cond_scale\n",
    "\n",
    "        \n",
    "        context = self.one_unet_in_gpu(unet = unet) if is_cuda and use_one_unet_in_gpu else nullcontext()\n",
    "\n",
    "        with context:\n",
    "\n",
    "            # low resolution conditioning\n",
    "            lowres_cond_img = lowres_noise_times = None\n",
    "\n",
    "            shape = (batch_size, channel, image_size, image_size)\n",
    "\n",
    "            resize_kwargs = dict()\n",
    "            \n",
    "            if unet.lowres_cond:\n",
    "                #lowres_noise_times = self.lowres_noise_schedule.get_times(batch_size, lowres_sample_noise_level, device = device)\n",
    "                lowres_noise_times = self.lowres_noise_schedule.get_times(img.shape[0], lowres_sample_noise_level, device = device)\n",
    "                lowres_cond_img = self.resize_to(img, image_size, **resize_kwargs)\n",
    "                lowres_cond_img, *_ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img,\n",
    "                                                                          t = lowres_noise_times, \n",
    "                                                                          noise = torch.randn_like(lowres_cond_img))\n",
    "\n",
    "            \n",
    "            shape = (lowres_cond_img.shape[0], self.channels, image_size, image_size)\n",
    "            \n",
    "            \n",
    "            img = self.p_sample_loop(\n",
    "                unet,\n",
    "                shape,\n",
    "                cond_scale = unet_cond_scale,\n",
    "                lowres_cond_img = lowres_cond_img,\n",
    "                lowres_noise_times = lowres_noise_times,\n",
    "                noise_scheduler = noise_scheduler,\n",
    "                pred_objective = pred_objective,\n",
    "                dynamic_threshold = dynamic_threshold,\n",
    "                use_tqdm = use_tqdm,\n",
    "            )\n",
    "\n",
    "            outputs.append(img)\n",
    "        \n",
    "        return outputs[-1]\n",
    "\n",
    "\n",
    "    @beartype\n",
    "    def p_losses(\n",
    "        self,\n",
    "        unet: Union[UNet],\n",
    "        x_start,\n",
    "        times,\n",
    "        *,\n",
    "        noise_scheduler,\n",
    "        lowres_cond_img = None,\n",
    "        lowres_aug_times = None,\n",
    "        noise = None,\n",
    "        times_next = None,\n",
    "        pred_objective = 'noise',\n",
    "        min_snr_gamma = None,\n",
    "        random_crop_size = None,\n",
    "        **kwargs\n",
    "        ):\n",
    "                \n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "\n",
    "        # get x_t\n",
    "        x_noisy, log_snr, alpha, sigma = noise_scheduler.q_sample(x_start = x_start, t = times, noise = noise)\n",
    "\n",
    "        \n",
    "        # also noise the lowres conditioning image\n",
    "        # at sample time, they then fix the noise level of 0.1 - 0.3\n",
    "        lowres_cond_img_noisy = None\n",
    "        if exists(lowres_cond_img):\n",
    "            lowres_aug_times = default(lowres_aug_times, times)\n",
    "            lowres_cond_img_noisy, *_ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img,\n",
    "                                                                            t = lowres_aug_times, \n",
    "                                                                            noise = torch.randn_like(lowres_cond_img))\n",
    "\n",
    "        # time condition\n",
    "        noise_cond = noise_scheduler.get_condition(times)\n",
    "\n",
    "        # unet kwargs\n",
    "        unet_kwargs = dict(\n",
    "                            lowres_noise_times = self.lowres_noise_schedule.get_condition(lowres_aug_times),\n",
    "                            lowres_cond_img = lowres_cond_img_noisy,\n",
    "                            cond_drop_prob = self.cond_drop_prob,\n",
    "                            **kwargs)\n",
    "\n",
    "        # get prediction\n",
    "        pred = unet.forward(\n",
    "                            x_noisy,\n",
    "                            noise_cond,\n",
    "                            **unet_kwargs)\n",
    "\n",
    "        # prediction objective\n",
    "        if pred_objective == 'noise':\n",
    "            target = noise\n",
    "        elif pred_objective == 'x_start':\n",
    "            target = x_start\n",
    "        elif pred_objective == 'v':\n",
    "            # derivation detailed in Appendix D of Progressive Distillation paper\n",
    "            # https://arxiv.org/abs/2202.00512\n",
    "            # this makes distillation viable as well as solve an issue with color shifting in upresoluting unets, noted in imagen-video\n",
    "            target = alpha * noise - sigma * x_start\n",
    "        else:\n",
    "            raise ValueError(f'unknown objective {pred_objective}')\n",
    "\n",
    "        # losses\n",
    "        losses = self.loss_fn(pred, target, reduction = 'none')\n",
    "        losses = reduce(losses, 'b ... -> b', 'mean')\n",
    "\n",
    "        # min snr loss reweighting\n",
    "        snr = log_snr.exp()\n",
    "        maybe_clipped_snr = snr.clone()\n",
    "\n",
    "        if exists(min_snr_gamma):\n",
    "            maybe_clipped_snr.clamp_(max = min_snr_gamma)\n",
    "\n",
    "        if pred_objective == 'noise':\n",
    "            loss_weight = maybe_clipped_snr / snr\n",
    "        elif pred_objective == 'x_start':\n",
    "            loss_weight = maybe_clipped_snr\n",
    "        elif pred_objective == 'v':\n",
    "            loss_weight = maybe_clipped_snr / (snr + 1)\n",
    "\n",
    "        losses = losses * loss_weight\n",
    "        return losses.mean()\n",
    "\n",
    "\n",
    "    @beartype\n",
    "    def forward(self\n",
    "                ,images\n",
    "                ,lowres_cond_img\n",
    "                ,**kwargs):\n",
    "        \n",
    "        unet = self.unets\n",
    "\n",
    "        images = cast_uint8_images_to_float(images)\n",
    "\n",
    "        assert images.dtype == torch.float or images.dtype == torch.half, f'images tensor needs to be floats but {images.dtype} dtype found instead'\n",
    "        \n",
    "        b, c, *_, h, w, device = *images.shape, images.device\n",
    "        \n",
    "        assert images.shape[1] == self.channels\n",
    "        assert h >= self.image_sizes and w >= self.image_sizes\n",
    "\n",
    "        times = self.noise_schedulers.sample_random_times(b, device = device)\n",
    "\n",
    "        \n",
    "        #prev_image_size = 64\n",
    "        #lowres_cond_img = self.resize_to(images, prev_image_size,  clamp_range = [-1,1] )\n",
    "        #lowres_cond_img = self.resize_to(lowres_cond_img, self.image_sizes, clamp_range = [-1,1] )\n",
    "\n",
    "        lowres_aug_time = self.lowres_noise_schedule.sample_random_times(1, device = device)\n",
    "        lowres_aug_times = repeat(lowres_aug_time, '1 -> b', b = b)\n",
    "        \n",
    "\n",
    "        images = self.resize_to(images, self.image_sizes)\n",
    "        \n",
    "        return self.p_losses(unet, images, times, \n",
    "                             noise_scheduler = self.noise_schedulers, lowres_cond_img = lowres_cond_img, \n",
    "                             lowres_aug_times = lowres_aug_times, pred_objective = self.pred_objectives, \n",
    "                             min_snr_gamma = self.min_snr_gamma, **kwargs)\n",
    "\n",
    "\n",
    "class Palette(BaseModel):\n",
    "    def __init__(self, networks, losses, sample_num, task, optimizers, log_iter,\n",
    "                 model_path, dataloader_circ_1, dataloader_circ_2, ema_scheduler=None,scale=1,  **kwargs):\n",
    "        ''' must to init BaseModel with kwargs '''\n",
    "        super(Palette, self).__init__(**kwargs)\n",
    "\n",
    "        ''' networks, dataloder, optimizers, losses, etc. '''\n",
    "        self.model_path = model_path\n",
    "        self.log_iter = log_iter\n",
    "        self.loss_fn = losses\n",
    "        self.netG = networks\n",
    "        self.dataloader_circ_1 = dataloader_circ_1\n",
    "        self.dataloader_circ_2 = dataloader_circ_2\n",
    "\n",
    "        if ema_scheduler is not None:\n",
    "            self.ema_scheduler = ema_scheduler\n",
    "            self.netG_EMA = copy.deepcopy(self.netG)\n",
    "            self.EMA = EMA(beta=self.ema_scheduler['ema_decay'])\n",
    "        else:\n",
    "            self.ema_scheduler = None\n",
    "        \n",
    "        ''' networks can be a list, and must convert by self.set_device function if using multiple GPU. '''\n",
    "        self.netG.to(self.device)\n",
    "        if self.ema_scheduler is not None:\n",
    "            self.netG_EMA = self.netG.to(self.device) \n",
    "        self.load_networks(self.model_path)\n",
    "\n",
    "        self.optG = torch.optim.Adam(list(filter(lambda p: p.requires_grad, self.netG.parameters())), **optimizers)\n",
    "        self.optimizers.append(self.optG)\n",
    "        self.resume_training() \n",
    "\n",
    "        #self.netG.set_loss(self.loss_fn)\n",
    "\n",
    "        self.sample_num = sample_num\n",
    "        self.task = task\n",
    "        self.scale=scale\n",
    "        \n",
    "    def set_input(self, data):\n",
    "        ''' must use set_device in tensor '''\n",
    "        self.cond_image = data.get('cond_image').to(self.device)\n",
    "        self.gt_image = data.get('gt_image').to(self.device)\n",
    "    \n",
    "        self.path = data['path']\n",
    "        self.batch_size = len(data['path'])\n",
    "    \n",
    "    def get_current_visuals(self, phase='train'):\n",
    "        dict = {\n",
    "            'gt_image': (self.gt_image.detach()[:].float().cpu()+1)/2,\n",
    "            'cond_image': (self.cond_image.detach()[:].float().cpu()+1)/2,\n",
    "        }\n",
    "\n",
    "        if phase != 'train':\n",
    "            dict.update({\n",
    "                'output': (self.output.detach()[:].float().cpu()+1)/2\n",
    "            })\n",
    "        return dict\n",
    "\n",
    "    def save_current_results(self):\n",
    "        ret_path = []\n",
    "        ret_result = []\n",
    "        for idx in range(self.batch_size):\n",
    "            ret_path.append('GT_{}'.format(self.path[idx]))\n",
    "            ret_result.append(self.gt_image[idx].detach().float().cpu())\n",
    "\n",
    "            ret_path.append('Process_{}'.format(self.path[idx]))\n",
    "            ret_result.append(self.visuals[idx::self.batch_size].detach().float().cpu())\n",
    "            \n",
    "            ret_path.append('Out_{}'.format(self.path[idx]))\n",
    "            ret_result.append(self.visuals[idx-self.batch_size].detach().float().cpu())\n",
    "        \n",
    "\n",
    "        self.results_dict = self.results_dict._replace(name=ret_path, result=ret_result)\n",
    "        return self.results_dict._asdict()\n",
    "\n",
    "\n",
    "    def train_step(self):\n",
    "        self.netG.train()\n",
    "        total_loss = 0.0  \n",
    "        \n",
    "        total_batches = len(self.phase_loader.dataset) // self.phase_loader.batch_size\n",
    "        pbar = tqdm(total=total_batches, position=0, leave=True)\n",
    "        for i, elements in enumerate(zip(self.phase_loader, self.dataloader_circ_1)):\n",
    "        \n",
    "            self.optG.zero_grad()\n",
    "            self.gt_image, self.cond_image_1 = elements\n",
    "            self.gt_image = self.gt_image.float().to(self.device)\n",
    "            self.cond_image = self.cond_image_1.float().to(self.device)\n",
    "            \n",
    "            self.cond_image,_ ,_ ,_  = imagen.noise_schedulers.q_sample(self.cond_image.to(\"cpu\"),torch.tensor(50))\n",
    "            self.cond_image = torch.clip(self.cond_image,-1,1).to(self.device)\n",
    "            \n",
    "\n",
    "            loss  = self.netG(self.gt_image, self.cond_image)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            self.optG.step()\n",
    "\n",
    "            # Accumulate total loss for the epoch\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            self.iter += self.batch_size\n",
    "\n",
    "            if self.ema_scheduler is not None:\n",
    "                if self.iter > self.ema_scheduler['ema_start'] and self.iter % self.ema_scheduler['ema_iter'] == 0:\n",
    "                    self.EMA.update_model_average(self.netG_EMA, self.netG)\n",
    "\n",
    "            for scheduler in self.schedulers:\n",
    "                scheduler.step()\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "\n",
    "        avg_loss = total_loss / len(self.phase_loader)\n",
    "        print(f\"Avg Loss for Epoch: {avg_loss}\")\n",
    "        wandb.log({\"loss\": avg_loss})\n",
    "\n",
    "        return avg_loss, self.cond_image, self.gt_image\n",
    "    \n",
    "    \n",
    "\n",
    "    def test(self, use_tqdm=False):\n",
    "        self.netG.eval()\n",
    "        with torch.no_grad():\n",
    "            total_batches = len(self.phase_loader.dataset) // self.phase_loader.batch_size\n",
    "            pbar = tqdm(total=total_batches, position=0, leave=True)\n",
    "            for i, elements in enumerate(zip(self.phase_loader, self.dataloader_circ_2)):\n",
    "                if i == 0:\n",
    "                    self.gt_image, self.cond_image_1  = elements\n",
    "                    break \n",
    "                \n",
    "            self.gt_image = self.gt_image.float().to(self.device)\n",
    "            self.cond_image_ = self.cond_image_1.unsqueeze(1).float().to(self.device)\n",
    "            \n",
    "            self.cond_image,_ ,_ ,_  = imagen.noise_schedulers.q_sample(self.cond_image_.to(\"cpu\"),torch.tensor(50))\n",
    "            self.cond_image = torch.clip(self.cond_image,-1,1).to(self.device)\n",
    "            \n",
    "            self.output = self.netG.restoration(start_image_or_video=self.cond_image, use_tqdm=use_tqdm)\n",
    "            \n",
    "            \n",
    "            \n",
    "            print(\"diffusion generated sample\")\n",
    "            plot_images_no_lab(self.output[:8])\n",
    "            print(\"condition sample\")\n",
    "            plot_images_no_lab(self.cond_image[:8])\n",
    "            print(\"original sample\")\n",
    "            plot_images_no_lab(self.gt_image[:8])\n",
    "\n",
    "\n",
    "            print(\"BLUE: ORIGINAL DATA VS ORANGE: DIFFUSION MODEL GENERATED\")\n",
    "            latitudinal_mean_three(self.gt_image.detach(),\n",
    "                             self.output.detach(),\n",
    "                             self.cond_image_.detach()\n",
    "                            ,label_name=[\"hr era5\",\"sr gfdl\",\"lr gfdl\"]) \n",
    "\n",
    "            print(\"BLUE: ORIGINAL DATA VS ORANGE: DIFFUSION MODEL GENERATED, GREEN: UNET GENERATED\")\n",
    "            histograms_three(self.gt_image.detach(),\n",
    "                             self.output.detach(),\n",
    "                             self.cond_image_.detach()\n",
    "                             ,xlim_end=None, var=\"p\"\n",
    "                             ,label_name=[\"hr era5\",\"sr gfdl\",\"lr gfdl\"])\n",
    "            \n",
    "            ssd = SpatialSpectralDensity_diff_res( \n",
    "                                     self.gt_image.detach().cpu().numpy()\n",
    "                                     ,self.output.detach().cpu().numpy()\n",
    "                                     ,self.cond_image_.detach().cpu().numpy()\n",
    "                                     ,new_labels = [\"hr era5\",\"sr gfdl\",\"lr gfdl\"])\n",
    "            ssd.run(num_times=None)\n",
    "            ssd.plot_psd(fname=f'/dss/dsshome1/0D/ge74xuf2/climate_diffusion/results/psd/psd_gfdl_era5_lr_vs_hr_vanilla.pdf'\n",
    "                         ,model_resolution=0.25,model_resolution_2=0.25)\n",
    "\n",
    "        return self.output\n",
    "\n",
    "\n",
    "    def load_networks(self, model_path):\n",
    "        \"\"\" save pretrained model and training state, which only do on GPU 0. \"\"\"\n",
    "        netG_label = self.netG.__class__.__name__\n",
    "        self.load_network(network=self.netG, network_label=netG_label, model_path=model_path, strict=False)\n",
    "        if self.ema_scheduler is not None:\n",
    "            self.load_network(network=self.netG_EMA, network_label=netG_label+'_ema',model_path=model_path, strict=False)\n",
    "          \n",
    "        \n",
    "    def load_pretrain_diffusion(self, model_path):\n",
    "        self.netG.load_state_dict(torch.load(model_path), strict=False)\n",
    "        self.netG.to(self.device)\n",
    "        \n",
    "        if self.ema_scheduler is not None:\n",
    "            self.netG_EMA.load_state_dict(torch.load(model_path), strict=False)\n",
    "            self.netG_EMA.to(self.device)\n",
    "            return self.netG_EMA\n",
    "        return self.netG\n",
    "        \n",
    "    \n",
    "    \n",
    "    def save_everything(self):\n",
    "        \"\"\" load pretrained model and training state. \"\"\"\n",
    "        netG_label = self.netG.__class__.__name__\n",
    "        self.save_network(network=self.netG, network_label=netG_label)\n",
    "        if self.ema_scheduler is not None:\n",
    "            self.save_network(network=self.netG_EMA, network_label=netG_label+'_ema')\n",
    "        self.save_training_state()\n",
    "\n",
    "CustomResult = collections.namedtuple('CustomResult', 'name result')\n",
    "\n",
    "\n",
    "def mse_loss(output, target):\n",
    "    return F.mse_loss(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cb5709c-8ea3-4563-aefd-881ce9ae8b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps 100\n",
      "Pretrained model in [] is not existed, Skip it\n",
      "Beign loading training states\n",
      "Training state in [.state] is not existed, Skip it\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\n",
    "    \"phase\": \"train\",\n",
    "    \"dataloader\": era5_hr_dl, \n",
    "    \"metrics\": [\"mae\"],\n",
    "    \"resume_state\" : \"\" ,#  \"/dss/dssfs04/lwp-dss-0002/pn49fu/pn49fu-dss-0002/ge74xuf2/diffusion/revision_100/100\",  \n",
    "    \"n_epochs\" : 100, \n",
    "    \"batch_size\" : config[\"batch_size\"],\n",
    "    \"save_checkpoint_epoch\" : 10, \n",
    "    \"save_path_base\":\"/dss/dssfs04/lwp-dss-0002/pn49fu/pn49fu-dss-0002/ge74xuf2/diffusion\",\n",
    "}\n",
    "\n",
    "\n",
    "imagen_unet = SRUnet256(num_resnet_blocks = (2, 4, 8, 8))\n",
    "\n",
    "imagen = Imagen(\n",
    "            unets = imagen_unet,\n",
    "            image_sizes = 256,\n",
    "            timesteps = 100,      \n",
    "            cond_drop_prob = 0, # is trained with 0 (no clf - g)\n",
    "            )\n",
    "\n",
    "\n",
    "palette_model = Palette(\n",
    "    networks=imagen,\n",
    "    losses=mse_loss,\n",
    "    sample_num=8,\n",
    "    task=\"inpainting\",\n",
    "    optimizers={\"lr\": 1e-4, \"weight_decay\": 0},  # was 5e-5\n",
    "    log_iter = 1000,                                                            \n",
    "    model_path = \"\" ,#  \"/dss/dssfs04/lwp-dss-0002/pn49fu/pn49fu-dss-0002/ge74xuf2/diffusion/revision_100/100_Imagen.pth\",    \n",
    "    dataloader_circ_1 = era5_lr_dl,  \n",
    "    dataloader_circ_2 = dataloader_bc_gfdl_val,\n",
    "    scale=0.5,\n",
    "    ema_scheduler = None,\n",
    "    #{\"ema_start\": 1,\n",
    "    #\"ema_iter\": 1,\n",
    "    #\"ema_decay\": 0.9999},\n",
    "    **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d969f854-ad6e-4663-a4db-6088bf4a7b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_training = False\n",
    "\n",
    "\n",
    "if do_training==True:\n",
    "    palette_model_result = palette_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d2e286-5fd5-4258-8bac-8a9672a4fb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_testing = False\n",
    "\n",
    "#if error - rmv: .unsqueeze(1) in test()\n",
    "if do_testing==True:\n",
    "    palette_model_result = palette_model.test(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a963791-9389-4542-a7df-0a44faa7c278",
   "metadata": {},
   "source": [
    "## Evaluate all era5 valid  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cd0da0-83d3-4a3e-a9f3-9b9d18662fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_save_sr_dataset_era5 = True\n",
    "\n",
    "bs_e_val = 1400\n",
    "\n",
    "if do_save_sr_dataset_era5==True:\n",
    "\n",
    "    era5_p_1d_256_v = era5_upscaled_1d_256(stage='valid')\n",
    "\n",
    "    dataloader_era5_val_1d_256 = data.DataLoader(era5_p_1d_256_v, batch_size=bs_e_val, shuffle=False, drop_last=True)\n",
    "\n",
    "    era5_lr = next(iter(dataloader_era5_val_1d_256))\n",
    "    print(era5_lr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc54e174-bcdd-4042-88e2-2b30edbdcae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_save_sr_dataset_era5==True:\n",
    "    era5_hr_ds = era5_0_25d_256(stage='valid')\n",
    "    era5_hr_dl = data.DataLoader(era5_hr_ds, batch_size=bs_e_val, shuffle=False, drop_last=True)\n",
    "    era5_hr_val = next(iter(era5_hr_dl))\n",
    "    print(\"HR ERA5\", era5_hr_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc88b525-eac0-44bd-9de8-037e06e885d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_save_sr_dataset_era5 = False\n",
    "\n",
    "m_path=\"/dss/dssfs04/lwp-dss-0002/pn49fu/pn49fu-dss-0002/ge74xuf2/diffusion/revision_100/100_Imagen.pth\"   \n",
    "dm_loaded = palette_model.load_pretrain_diffusion(m_path)\n",
    "\n",
    "if do_save_sr_dataset_era5==True:\n",
    "    output_tensors = []\n",
    "\n",
    "    for b, el in enumerate(dataloader_era5_val_1d_256):\n",
    "        print(b)\n",
    "        era5_lr_batch = el.to(\"cuda\").float()         \n",
    "        \n",
    "        sr_era5_output = dm_loaded.restoration(start_image_or_video=era5_lr_batch.to(\"cuda\").float() )\n",
    "        plot_images_no_lab(era5_lr_batch[:7])\n",
    "        plot_images_no_lab(sr_era5_output[:7])\n",
    "        \n",
    "        ssim_list = []\n",
    "        for i in range(sr_era5_output.size(0)):  \n",
    "            sr_image = sr_era5_output.cpu()[i]\n",
    "            bc_image = era5_lr_batch.cpu()[i]\n",
    "            ssim_val = ssim(sr_image[0].numpy(), bc_image[0].numpy(), data_range=2.0).item()\n",
    "            ssim_list.append(ssim_val)  \n",
    "        #print(\"SSIM values\", ssim_list)\n",
    "        print(\"average SSIM\", np.sum(ssim_list)/len(ssim_list))\n",
    "            \n",
    "        output_tensors.append(sr_era5_output)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ssd = SpatialSpectralDensity_diff_res( era5_hr_ds.inverse_dwd_trafo(era5_hr_val).numpy(), \n",
    "                                                 era5_hr_ds.inverse_dwd_trafo(sr_era5_output.cpu()).numpy(),\n",
    "                                                 era5_lr_ds.inverse_dwd_trafo(era5_lr_batch.cpu()).numpy(),\n",
    "                                                 new_labels = [\"ear5 hr\",\"sr era5\",\"era5 lr\"])\n",
    "        ssd.run(num_times=None)\n",
    "        ssd.plot_psd(fname=\"\",model_resolution=0.25, model_resolution_2 = 0.25)\n",
    "        \n",
    "        latitudinal_mean_three(era5_hr_ds.inverse_dwd_trafo(era5_hr_val)\n",
    "                             ,era5_hr_ds.inverse_dwd_trafo(sr_era5_output.cpu())\n",
    "                             ,era5_lr_ds.inverse_dwd_trafo(era5_lr_batch.cpu())\n",
    "                             ,label_name = [\"ear5 hr\",\"sr era5\",\"era5 lr\"])\n",
    "\n",
    "    sr_era5_dataset = torch.cat(output_tensors, dim=0) \n",
    "    \n",
    "    print(sr_era5_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d9106a-9082-4c22-9bdc-ee0c9a53f435",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_save_sr_dataset_era5==True:\n",
    "    sr_era5_dataset = torch.cat(output_tensors, dim=0) \n",
    "    sr_era5_dataset.shape\n",
    "\n",
    "if do_save_sr_dataset_era5==True:\n",
    "    \n",
    "    save_path = 'data/100_era5_val_sr_imagen_20_09.pth'\n",
    "    torch.save(sr_era5_dataset, save_path)\n",
    "    print(\"saving to:\",save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2375f9-cb62-445d-afc9-9b518b4eb4c8",
   "metadata": {},
   "source": [
    "# GFDL valid eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8e8c02-b3f5-458d-9e5c-fe7837508b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_eval_sr_dataset = True\n",
    "\n",
    "if do_eval_sr_dataset == True:\n",
    "    bc_qm_gfdl_dataset = QM_GFDL_LR_Dataset_256('data/11_01_deltaQM_debiased_gfdl_valid_custom_dl.pth')\n",
    "\n",
    "    dataloader_embed_gfdl = data.DataLoader(bc_qm_gfdl_dataset, batch_size=1400, shuffle=False,\n",
    "                                      drop_last=True,num_workers=2)\n",
    "\n",
    "\n",
    "    embed_gfdl = next(iter(dataloader_embed_gfdl))\n",
    "    print(\"embedded gfdl shape:\",embed_gfdl.shape)\n",
    "\n",
    "if do_eval_sr_dataset == True:\n",
    "    era5_hr_ds = era5_0_25d_256(stage='valid')\n",
    "    era5_hr_dl = data.DataLoader(era5_hr_ds, batch_size=1400, shuffle=False, drop_last=True)\n",
    "    era5_hr_val = next(iter(era5_hr_dl))\n",
    "    print(\"HR ERA5\", era5_hr_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badc8f7d-5969-4a63-894d-ac2455a5a1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_save_sr_dataset = False\n",
    "\n",
    "m_path=\"/dss/dssfs04/lwp-dss-0002/pn49fu/pn49fu-dss-0002/ge74xuf2/diffusion/revision_100/100_Imagen.pth\"   \n",
    "dm_loaded = palette_model.load_pretrain_diffusion(m_path)\n",
    "\n",
    "if do_save_sr_dataset==True:\n",
    "    output_tensors = []\n",
    "    \n",
    "    \n",
    "    for b, el in enumerate(dataloader_embed_gfdl):\n",
    "        print(b)\n",
    "        gfdl_qm_ = el.to(\"cuda\").float()   # was .unsqueeze(1)\n",
    "        \n",
    "        gfdl_qm,_ ,_ ,_  = imagen.noise_schedulers.q_sample(gfdl_qm_.to(\"cpu\"),torch.tensor(50))\n",
    "        gfdl_qm = torch.clip(gfdl_qm,-1,1)\n",
    "        \n",
    "        \n",
    "        sr_gfdl_output = dm_loaded.restoration(start_image_or_video=gfdl_qm.to(\"cuda\").float(), use_tqdm=False)\n",
    "        plot_images_no_lab(gfdl_qm_[:7])\n",
    "        plot_images_no_lab(sr_gfdl_output[:7])\n",
    "        \n",
    "        ssim_list = []\n",
    "        for i in range(sr_gfdl_output.size(0)):  \n",
    "            sr_image = sr_gfdl_output.cpu()[i]\n",
    "            bc_image = gfdl_qm_.cpu()[i]\n",
    "            ssim_val = ssim(sr_image[0].numpy(), bc_image[0].numpy(), data_range=2.0).item()\n",
    "            ssim_list.append(ssim_val)  \n",
    "        #print(\"SSIM values\", ssim_list)\n",
    "        print(\"average SSIM\", np.sum(ssim_list)/len(ssim_list))\n",
    "            \n",
    "        output_tensors.append(sr_gfdl_output)\n",
    "        \n",
    "        \n",
    "        ssd = SpatialSpectralDensity_diff_res( era5_hr_ds.inverse_dwd_trafo(era5_hr_val).numpy(), \n",
    "                                                 era5_hr_ds.inverse_dwd_trafo(sr_gfdl_output.cpu()).numpy(),\n",
    "                                                 era5_lr_ds.inverse_dwd_trafo(gfdl_qm_.cpu()).numpy(),\n",
    "                                                 new_labels = [\"ear5 hr\",\"sr bc gfdl\",\"qm gfdl lr\"])\n",
    "        ssd.run(num_times=None)\n",
    "        ssd.plot_psd(fname='',model_resolution=0.25, model_resolution_2 = 0.25)\n",
    "        \n",
    "        latitudinal_mean_three(era5_hr_ds.inverse_dwd_trafo(era5_hr_val)\n",
    "                             ,era5_hr_ds.inverse_dwd_trafo(sr_gfdl_output.cpu())\n",
    "                             ,era5_lr_ds.inverse_dwd_trafo(gfdl_qm_.cpu())\n",
    "                             ,label_name = [\"ear5 hr\",\"sr bc gfdl\",\"qm gfdl lr\"])\n",
    "\n",
    "\n",
    "    sr_gfdl_dataset = torch.cat(output_tensors, dim=0) \n",
    "    \n",
    "    print(sr_gfdl_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce51b69-fd7a-40fc-9dab-cb9ed31a6515",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_save_sr_dataset == True:\n",
    "    sr_gfdl_dataset = torch.cat(output_tensors, dim=0) \n",
    "    print(sr_gfdl_dataset.shape)\n",
    "\n",
    "if do_save_sr_dataset==True:\n",
    "    \n",
    "    save_path = 'data/100_gfdl_sr_imagen_e100_6_09.pth'\n",
    "    torch.save(sr_gfdl_dataset, save_path)\n",
    "    print(\"saving to:\",save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
