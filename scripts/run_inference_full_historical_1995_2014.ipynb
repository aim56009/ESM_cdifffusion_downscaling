{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d62ec5ab-4491-4d9f-9d14-a507ed23b57d",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51482e99-1c94-4153-aae2-bcf85f323a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install seaborn\n",
    "\n",
    "!pip install xarray\n",
    "!pip install netcdf4\n",
    "!pip install collections\n",
    "!pip install scikit-image\n",
    "!pip install pysteps\n",
    "!pip install cartopy\n",
    "!pip install beartype\n",
    "!pip install pysteps\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d23b3428-299e-446c-aa57-40ff1db10b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/dss/dsshome1/0D/ge74xuf2/climate_diffusion'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "new_directory = '/dss/dsshome1/0D/ge74xuf2/climate_diffusion'\n",
    "os.chdir(new_directory)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "befc82b3-dfd3-43a4-90c5-730df0baf47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pysteps configuration file found at: /usr/local/lib/python3.10/dist-packages/pysteps/pystepsrc\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import wandb\n",
    "import IPython.display as display\n",
    "import logging\n",
    "import torch.nn.functional as F\n",
    "import collections\n",
    "import copy\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import random_split\n",
    "from inspect import isfunction\n",
    "from functools import partial\n",
    "from abc import abstractmethod\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from pysteps.utils.spectral import rapsd, corrcoef\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import tqdm\n",
    "#from src.psd_utils import SpatialSpectralDensity_4_diff_res, SpatialSpectralDensity_diff_res\n",
    "\n",
    "from src.utils import *\n",
    "from src.helper import *\n",
    "from src.utils_essential import *\n",
    "\n",
    "\n",
    "from src.base_network import BaseNetwork\n",
    "\n",
    "\n",
    "\n",
    "from src.imagen_unet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7bb2603-aaeb-4dc5-9322-fd8264e9c928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataloader_sr import gfdl_eval_256, era5_upscaled_1d_256, era5_0_25d_256, qm_gfdl_trafo_units_hr\n",
    "from src.dataloader_sr import QM_GFDL_LR_Dataset_256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8169898-4fbb-408a-8e1f-d60e9b3cfb32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmichi\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/dss/dsshome1/0D/ge74xuf2/climate_diffusion/wandb/run-20250415_141341-vv8ged7f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/michi/climate-diffusion/runs/vv8ged7f' target=\"_blank\">worldly-capybara-2232</a></strong> to <a href='https://wandb.ai/michi/climate-diffusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/michi/climate-diffusion' target=\"_blank\">https://wandb.ai/michi/climate-diffusion</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/michi/climate-diffusion/runs/vv8ged7f' target=\"_blank\">https://wandb.ai/michi/climate-diffusion/runs/vv8ged7f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR ERA5 torch.Size([4, 1, 256, 256])\n",
      "LR ERA5 torch.Size([4, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "config = {\"run_name\": \"bc_sr_imagen_dn1000\",     \n",
    "          \"epochs\":        400,\n",
    "          \"batch_size\":    4, \n",
    "          \"lr\":            1e-5, \n",
    "          \"image_size\":    256,             \n",
    "          \"device\":        \"cuda\", \n",
    "          \"num_workers\":   8, \n",
    "}\n",
    "#wandb.config.update({\"batch_size\": 32})\n",
    "\n",
    "wandb.init(project='climate-diffusion', entity='Michi',config=config, save_code=True)\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", level=logging.INFO, datefmt=\"%I:%M:%S\")\n",
    "\n",
    "# # Dataloaders \n",
    "\n",
    "# +\n",
    "era5_hr_ds = era5_0_25d_256(stage='train')\n",
    "\n",
    "\n",
    "era5_hr_dl = data.DataLoader(era5_hr_ds, batch_size=wandb.config.batch_size, shuffle=False, drop_last=True,\n",
    "                                     num_workers=wandb.config.num_workers)\n",
    "\n",
    "sample_era5_025_tr = next(iter(era5_hr_dl))\n",
    "print(\"HR ERA5\", sample_era5_025_tr.shape)\n",
    "\n",
    "# +\n",
    "era5_lr_ds = era5_upscaled_1d_256(stage='train')\n",
    "\n",
    "era5_lr_dl = data.DataLoader(era5_lr_ds, batch_size=wandb.config.batch_size, shuffle=False, drop_last=True,\n",
    "                                     num_workers=wandb.config.num_workers)\n",
    "\n",
    "sample_era5_1d_256p = next(iter(era5_lr_dl))\n",
    "print(\"LR ERA5\", sample_era5_1d_256p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faea525-e306-4173-b6a0-5ce7ce91d677",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1efa426b-67dc-4481-9f30-bdcd4a38dd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps 100\n",
      "Loading pretrained model from [/dss/dssfs04/lwp-dss-0002/pn49fu/pn49fu-dss-0002/ge74xuf2/diffusion/revision_100/100_Imagen.pth] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1619518/2110193946.py:300: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  network.load_state_dict(torch.load(model_path), strict=strict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beign loading training states\n",
      "Loading training state for [/dss/dssfs04/lwp-dss-0002/pn49fu/pn49fu-dss-0002/ge74xuf2/diffusion/revision_100/100.state] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1619518/2110193946.py:330: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  resume_state = torch.load(state_path)#.to(self.device)\n"
     ]
    }
   ],
   "source": [
    "def first(arr, d = None):\n",
    "    if len(arr) == 0:\n",
    "        return d\n",
    "    return arr[0]\n",
    "\n",
    "def resize_image_to(\n",
    "    image,\n",
    "    target_image_size,\n",
    "    clamp_range = None,\n",
    "    mode = 'nearest'):\n",
    "    orig_image_size = image.shape[-1]\n",
    "\n",
    "    if orig_image_size == target_image_size:\n",
    "        return image\n",
    "\n",
    "    out = F.interpolate(image, target_image_size, mode = mode)\n",
    "\n",
    "    if exists(clamp_range):\n",
    "        out = out.clamp(*clamp_range)\n",
    "\n",
    "    return out\n",
    "\n",
    "def eval_decorator(fn):\n",
    "    def inner(model, *args, **kwargs):\n",
    "        was_training = model.training\n",
    "        model.eval()\n",
    "        out = fn(model, *args, **kwargs)\n",
    "        model.train(was_training)\n",
    "        return out\n",
    "    return inner\n",
    "\n",
    "def module_device(module):\n",
    "    return next(module.parameters()).device\n",
    "\n",
    "def cast_uint8_images_to_float(images):\n",
    "    if not images.dtype == torch.uint8:\n",
    "        return images\n",
    "    return images / 255\n",
    "\n",
    "\n",
    "# -\n",
    "\n",
    "# # Diffusion\n",
    "\n",
    "class GaussianDiffusionContinuousTimes(BaseNetwork):\n",
    "    def __init__(self, *, noise_schedule, timesteps = 1000,**kwargs):\n",
    "        super(GaussianDiffusionContinuousTimes, self).__init__(**kwargs)\n",
    "        \n",
    "        if noise_schedule == \"linear\":\n",
    "            self.log_snr = beta_linear_log_snr\n",
    "        elif noise_schedule == \"cosine\":\n",
    "            self.log_snr = alpha_cosine_log_snr\n",
    "        else:\n",
    "            raise ValueError(f'invalid noise schedule {noise_schedule}')\n",
    "\n",
    "        self.num_timesteps = timesteps\n",
    "\n",
    "    def get_times(self, batch_size, noise_level, *, device):\n",
    "        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n",
    "\n",
    "    def sample_random_times(self, batch_size, *, device):\n",
    "        return torch.zeros((batch_size,), device = device).float().uniform_(0, 1)\n",
    "\n",
    "    def get_condition(self, times):\n",
    "        return maybe(self.log_snr)(times)\n",
    "\n",
    "    def get_sampling_timesteps(self, batch, *, device):\n",
    "        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n",
    "        times = repeat(times, 't -> b t', b = batch)\n",
    "        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n",
    "        times = times.unbind(dim = -1)\n",
    "        return times\n",
    "\n",
    "    def q_posterior(self, x_start, x_t, t, *, t_next = None):\n",
    "        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n",
    "\n",
    "        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n",
    "        log_snr = self.log_snr(t)\n",
    "        log_snr_next = self.log_snr(t_next)\n",
    "        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n",
    "\n",
    "        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n",
    "        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n",
    "\n",
    "        # c - as defined near eq 33\n",
    "        c = -expm1(log_snr - log_snr_next)\n",
    "        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n",
    "        \n",
    "        '''\n",
    "        alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n",
    "        alpha_next * ( (x_t - x_t*c) / alpha + c * x_start)\n",
    "        alpha_next * ( x_t/ alpha - c*x_t/ alpha + c * x_start)\n",
    "        x_t*alpha_next/ alpha - c*x_t*alpha_next/ alpha + c * alpha_next*x_start)\n",
    "        alpha_next/ alpha ( x_t - c * x_t + c * alpha * x_start)      witht  x_start = (x_t -simga*noise)/alpha\n",
    "        alpha_next/alpha (x_t - c* x_t + c * alpha *[(x_t-simga*noise)/alpha])\n",
    "        alpha_next/alpha (x_t - c* x_t + c*x_t - c*simga*noise)\n",
    "        alpha_next/alpha (x_t - c*simga*noise)\n",
    "        alpha_next/alpha (x_t + c) (- alpha/alpha_next * sigma * noise)\n",
    "        '''\n",
    "        \n",
    "        # following (eq. 33)\n",
    "        posterior_variance = (sigma_next ** 2) * c\n",
    "        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    def q_sample(self, x_start, t, noise = None):\n",
    "        dtype = x_start.dtype\n",
    "\n",
    "        if isinstance(t, float):\n",
    "            batch = x_start.shape[0]\n",
    "            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n",
    "\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "        log_snr = self.log_snr(t).type(dtype)\n",
    "        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n",
    "        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n",
    "\n",
    "        return alpha * x_start + sigma * noise, log_snr, alpha, sigma\n",
    "\n",
    "    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n",
    "        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n",
    "        batch = shape[0]\n",
    "\n",
    "        if isinstance(from_t, float):\n",
    "            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n",
    "\n",
    "        if isinstance(to_t, float):\n",
    "            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n",
    "\n",
    "        noise = default(noise, lambda: torch.randn_like(x_from))\n",
    "\n",
    "        log_snr = self.log_snr(from_t)\n",
    "        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n",
    "        alpha, sigma = log_snr_to_alpha_sigma(log_snr_padded_dim)\n",
    "\n",
    "        log_snr_to = self.log_snr(to_t)\n",
    "        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n",
    "        alpha_to, sigma_to = log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n",
    "\n",
    "        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n",
    "\n",
    "    def predict_start_from_v(self, x_t, t, v):\n",
    "        log_snr = self.log_snr(t)\n",
    "        log_snr = right_pad_dims_to(x_t, log_snr)\n",
    "        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n",
    "        return alpha * x_t - sigma * v\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        log_snr = self.log_snr(t)\n",
    "        log_snr = right_pad_dims_to(x_t, log_snr)\n",
    "        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n",
    "        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n",
    "        #x_0 * alpha = x_t -simga*noise\n",
    "        #noise = (x_t - x_0*alpha)/sigma\n",
    "\n",
    "\n",
    "# +\n",
    "#diffusion_network = Diffusion_SR(imagen_unet)\n",
    "#gaussian_diffusion = GaussianDiffusionContinuousTimes(noise_schedule=\"linear\", timesteps=10)\n",
    "# -\n",
    "\n",
    "# # Training\n",
    "\n",
    "class EMA():\n",
    "    def __init__(self, beta=0.9999):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "    def update_model_average(self, ma_model, current_model):\n",
    "        for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n",
    "            old_weight, up_weight = ma_params.data, current_params.data\n",
    "            ma_params.data = self.update_average(old_weight, up_weight)\n",
    "    def update_average(self, old, new):\n",
    "        if old is None:\n",
    "            return new\n",
    "        return old * self.beta + (1 - self.beta) * new\n",
    "\n",
    "\n",
    "class BaseModel():\n",
    "    def __init__(self, phase,  dataloader, metrics, n_epochs=10, batch_size = 8, \n",
    "                  save_checkpoint_epoch=10,resume_state=False,\n",
    "                 save_path_base=\"/dss/dssfs04/lwp-dss-0002/pn49fu/pn49fu-dss-0002/ge74xuf2/diffusion\", **kwargs):\n",
    "        \"\"\" init model with basic input, which are from __init__(**kwargs) function in inherited class \"\"\"\n",
    "        self.phase = phase\n",
    "        self.device = config['device']\n",
    "        \n",
    "        self.n_epochs = n_epochs\n",
    "        self.resume_state = resume_state\n",
    "        self.save_checkpoint_epoch = save_checkpoint_epoch\n",
    "\n",
    "        ''' optimizers and schedulers '''\n",
    "        self.schedulers = []\n",
    "        self.optimizers = []\n",
    "\n",
    "        ''' process record '''\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch = 0\n",
    "        self.iter = 0 \n",
    "\n",
    "        self.phase_loader = dataloader\n",
    "        self.metrics = metrics\n",
    "\n",
    "        ''' logger to log file, which only work on GPU 0. writer to tensorboard and result file '''\n",
    "        self.results_dict = CustomResult([],[]) # {\"name\":[], \"result\":[]}\n",
    "        \n",
    "        self.save_path_base = save_path_base\n",
    "\n",
    "    \n",
    "\n",
    "   \n",
    "    def train(self):\n",
    "        while self.epoch <= self.n_epochs: \n",
    "            self.epoch += 1\n",
    "            \n",
    "            train_log, condition_after_unet, original_img = self.train_step()\n",
    "            \n",
    "            print(\"epoch:\", self.epoch)\n",
    "            \n",
    "            if self.epoch % 1 == 0:\n",
    "                output_sampled = imagen.restoration(start_image_or_video=condition_after_unet.to(config[\"device\"]))\n",
    "                \n",
    "                output_sampled_test = wandb.Image(output_sampled)\n",
    "                wandb.log({\"diffusion gen img\": output_sampled_test})\n",
    "                condition_after_unet_wb = wandb.Image(condition_after_unet)\n",
    "                wandb.log({\"condition img\": condition_after_unet_wb})\n",
    "                original_img_wb = wandb.Image(original_img)\n",
    "                wandb.log({\"original img\": original_img_wb})\n",
    "                \n",
    "                print(\"proxy-ERA5\")\n",
    "                plot_images_no_lab(condition_after_unet)\n",
    "                print(\"SR proxy-ERA5\")\n",
    "                plot_images_no_lab(output_sampled)\n",
    "                print(\"ERA5\")\n",
    "                plot_images_no_lab(original_img)\n",
    "                \n",
    "                \n",
    "                print(\"BLUE: ORIGINAL DATA VS ORANGE: DIFFUSION MODEL GENERATED\")\n",
    "                latitudinal_mean_three(original=original_img, generated=output_sampled, \n",
    "                                       label=condition_after_unet.detach() , var=\"p\")\n",
    "                print(\"BLUE: ORIGINAL DATA VS ORANGE: DIFFUSION MODEL GENERATED, GREEN: UNET GENERATED\")\n",
    "                histograms_three(original=original_img.detach(), generated=output_sampled.detach(),\n",
    "                                 label= condition_after_unet.detach(),xlim_end=None, var=\"p\")\n",
    "                \n",
    "                \n",
    "                ssd = SpatialSpectralDensity_diff_res( \n",
    "                                      original_img.detach().cpu().numpy()\n",
    "                                     ,output_sampled.detach().cpu().numpy()\n",
    "                                     ,condition_after_unet.detach().cpu().numpy()\n",
    "                                     ,new_labels = [\"era5 hr\",\" sr era5\",\"era5 lr\"])\n",
    "                ssd.run(num_times=None)\n",
    "                ssd.plot_psd(fname=f'/dss/dsshome1/0D/ge74xuf2/climate_diffusion/results/psd/psd_gfdl_era5_lr_vs_hr_vanilla.pdf'\n",
    "                             ,model_resolution=0.25,model_resolution_2=0.25)\n",
    "                \n",
    "                plt.show()\n",
    "                  \n",
    "                    \n",
    "            if self.epoch % self.save_checkpoint_epoch == 0:\n",
    "                print('Saving the self at the end of epoch {:.0f}'.format(self.epoch))\n",
    "                self.save_everything()\n",
    "\n",
    "    def test(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_step(self):\n",
    "        raise NotImplementedError('You must specify how to train your networks.')\n",
    "\n",
    "\n",
    "    def test_step(self):\n",
    "        pass\n",
    "    \n",
    "    def print_network(self, network):\n",
    "        \"\"\" print network structure, only work on GPU 0 \"\"\"\n",
    "        if isinstance(network, nn.DataParallel) or isinstance(network, nn.parallel.DistributedDataParallel):\n",
    "            network = network.module\n",
    "        \n",
    "        s, n = str(network), sum(map(lambda x: x.numel(), network.parameters()))\n",
    "        net_struc_str = '{}'.format(network.__class__.__name__)\n",
    "        print('Network structure: {}, with parameters: {:,d}'.format(net_struc_str, n))\n",
    "        print(s)\n",
    "\n",
    "    def save_network(self, network, network_label):\n",
    "        \"\"\" save network structure, only work on GPU 0 \"\"\"\n",
    "        save_filename = '{}_{}.pth'.format(self.epoch, network_label)\n",
    "        #save_path = os.path.join(os.path.join(\"models\", config['run_name'], save_filename))\n",
    "        save_path = os.path.join(self.save_path_base,  config['run_name'],save_filename)\n",
    "        \n",
    "        if isinstance(network, nn.DataParallel) or isinstance(network, nn.parallel.DistributedDataParallel):\n",
    "            network = network.module\n",
    "        state_dict = network.state_dict()\n",
    "        for key, param in state_dict.items():\n",
    "            state_dict[key] = param.cpu()\n",
    "        torch.save(state_dict, save_path)\n",
    "\n",
    "    def load_network(self, network, network_label, model_path, strict=True):        \n",
    "        if not os.path.exists(model_path):\n",
    "            print('Pretrained model in [{:s}] is not existed, Skip it'.format(model_path))\n",
    "            return\n",
    "\n",
    "        print('Loading pretrained model from [{:s}] ...'.format(model_path))\n",
    "        network.load_state_dict(torch.load(model_path), strict=strict)\n",
    "        network.to(self.device)\n",
    "\n",
    "    def save_training_state(self):\n",
    "        \"\"\" saves training state during training, only work on GPU 0 \"\"\"\n",
    "\n",
    "        assert isinstance(self.optimizers, list) and isinstance(self.schedulers, list), 'optimizers and schedulers must be a list.'\n",
    "        state = {'epoch': self.epoch, 'iter': self.iter, 'schedulers': [], 'optimizers': []}\n",
    "        for s in self.schedulers:\n",
    "            state['schedulers'].append(s.state_dict())\n",
    "        for o in self.optimizers:\n",
    "            state['optimizers'].append(o.state_dict())\n",
    "        save_filename = '{}.state'.format(self.epoch)\n",
    "        save_path = os.path.join(os.path.join(self.save_path_base, config['run_name'], save_filename))\n",
    "        torch.save(state, save_path)\n",
    "\n",
    "    def resume_training(self):\n",
    "        \"\"\" resume the optimizers and schedulers for training, only work when phase is test or resume training enable \"\"\"\n",
    "        if self.phase!='train' or self.resume_state is None:\n",
    "            return\n",
    "        print('Beign loading training states'.format())\n",
    "        assert isinstance(self.optimizers, list) and isinstance(self.schedulers, list), 'optimizers and schedulers must be a list.'\n",
    "        \n",
    "        state_path = \"{}.state\".format(self.resume_state)\n",
    "        \n",
    "        if not os.path.exists(state_path):\n",
    "            print('Training state in [{:s}] is not existed, Skip it'.format(state_path))\n",
    "            return\n",
    "\n",
    "        print('Loading training state for [{:s}] ...'.format(state_path))\n",
    "        resume_state = torch.load(state_path)#.to(self.device) \n",
    "        \n",
    "        resume_optimizers = resume_state['optimizers']\n",
    "        resume_schedulers = resume_state['schedulers']\n",
    "        assert len(resume_optimizers) == len(self.optimizers), 'Wrong lengths of optimizers {} != {}'.format(len(resume_optimizers), len(self.optimizers))\n",
    "        assert len(resume_schedulers) == len(self.schedulers), 'Wrong lengths of schedulers {} != {}'.format(len(resume_schedulers), len(self.schedulers))\n",
    "        for i, o in enumerate(resume_optimizers):\n",
    "            self.optimizers[i].load_state_dict(o)\n",
    "        for i, s in enumerate(resume_schedulers):\n",
    "            self.schedulers[i].load_state_dict(s)\n",
    "\n",
    "        self.epoch = resume_state['epoch']\n",
    "        self.iter = resume_state['iter']\n",
    "\n",
    "    def load_everything(self):\n",
    "        pass \n",
    "    \n",
    "    @abstractmethod\n",
    "    def save_everything(self):\n",
    "        raise NotImplementedError('You must specify how to save your networks, optimizers and schedulers.')\n",
    "\n",
    "\n",
    "class Imagen(BaseNetwork): \n",
    "    def __init__(\n",
    "        self,\n",
    "        unets,\n",
    "        *,\n",
    "        image_sizes,                                # for cascading ddpm, image size at each stage\n",
    "        channels = 1,\n",
    "        timesteps = 1000,\n",
    "        cond_drop_prob = 0.1,\n",
    "        loss_type = 'l2',\n",
    "        noise_schedules = 'cosine',\n",
    "        pred_objectives = 'v',\n",
    "        lowres_noise_schedule = 'linear',\n",
    "        lowres_sample_noise_level = 0.2,            # in the paper, they present a new trick where they noise the lowres conditioning image, and at sample time, fix it to a certain level (0.1 or 0.3) - the unets are also made to be conditioned on this noise level\n",
    "        dynamic_thresholding = True,\n",
    "        dynamic_thresholding_percentile = 0.95,     # unsure what this was based on perusal of paper\n",
    "        resize_mode = 'nearest',\n",
    "        min_snr_loss_weight = True,                 # https://arxiv.org/abs/2303.09556\n",
    "        min_snr_gamma = 5\n",
    "        ,**kwargs):\n",
    "\n",
    "        #super(Imagen).__init__( **kwargs)\n",
    "        super(Imagen, self).__init__(**kwargs)\n",
    "        \n",
    "\n",
    "        # loss \n",
    "        self.loss_type = loss_type\n",
    "        self.loss_fn = F.mse_loss\n",
    "\n",
    "        # channels\n",
    "        self.channels = channels\n",
    "        self.noise_schedulers = GaussianDiffusionContinuousTimes(noise_schedule = noise_schedules, timesteps=timesteps)\n",
    "        print(\"timesteps\",self.noise_schedulers.num_timesteps)\n",
    "        \n",
    "        # lowres augmentation noise schedule\n",
    "        self.lowres_noise_schedule = GaussianDiffusionContinuousTimes(noise_schedule = lowres_noise_schedule )\n",
    "\n",
    "        # ddpm objectives - predicting noise by default\n",
    "        self.pred_objectives = pred_objectives\n",
    "\n",
    "\n",
    "        # construct unets\n",
    "\n",
    "        self.unets = unets.cast_model_parameters(lowres_cond = True,\n",
    "                                                channels = self.channels,\n",
    "                                                channels_out = 1)\n",
    "            \n",
    "        # unet image sizes\n",
    "\n",
    "        self.image_sizes = image_sizes\n",
    "        self.sample_channels = self.channels\n",
    "        self.resize_to = resize_image_to\n",
    "\n",
    "\n",
    "        # cascading ddpm related stuff\n",
    "\n",
    "        lowres_conditions = lambda t: t.lowres_cond\n",
    "        self.lowres_sample_noise_level = lowres_sample_noise_level\n",
    "\n",
    "        # classifier free guidance\n",
    "\n",
    "        self.cond_drop_prob = cond_drop_prob\n",
    "        self.can_classifier_guidance = cond_drop_prob > 0.\n",
    "\n",
    "        # dynamic thresholding\n",
    "\n",
    "        self.dynamic_thresholding = dynamic_thresholding\n",
    "        self.dynamic_thresholding_percentile = dynamic_thresholding_percentile\n",
    "\n",
    "        # min snr loss weight\n",
    "\n",
    "        min_snr_loss_weight = min_snr_loss_weight\n",
    "        min_snr_gamma = min_snr_gamma\n",
    "\n",
    "        self.min_snr_gamma = min_snr_gamma if min_snr_loss_weight else None\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def device(self):\n",
    "        return self._temp.device\n",
    "\n",
    "\n",
    "    def reset_unets_all_one_device(self, device = None):\n",
    "        device = default(device, \"cuda\")\n",
    "        self.unets.to(device)\n",
    "\n",
    "        self.unet_being_trained_index = -1\n",
    "\n",
    "    @contextmanager\n",
    "    def one_unet_in_gpu(self,  unet = None):\n",
    "        cpu = torch.device('cpu')\n",
    "        devices = module_device(unet) \n",
    "        self.unets.to(cpu)\n",
    "        unet.to(\"cuda\")\n",
    "        yield\n",
    "        unet.to(devices)\n",
    "\n",
    "    # gaussian diffusion methods\n",
    "    def p_mean_variance(\n",
    "        self,\n",
    "        unet,\n",
    "        x,\n",
    "        t,\n",
    "        *,\n",
    "        noise_scheduler,\n",
    "        lowres_cond_img = None,\n",
    "        lowres_noise_times = None,\n",
    "        cond_scale = 1.,\n",
    "        model_output = None,\n",
    "        t_next = None,\n",
    "        pred_objective = 'noise',\n",
    "        dynamic_threshold = True):\n",
    "        \n",
    "        assert not (cond_scale != 1. and not self.can_classifier_guidance), 'imagen was not trained with conditional dropout, and thus one cannot use classifier free guidance (cond_scale anything other than 1)'\n",
    "\n",
    "\n",
    "        pred = default(model_output, lambda: unet.forward(\n",
    "            x,\n",
    "            noise_scheduler.get_condition(t),\n",
    "            #cond_scale = cond_scale,                # for classifier free guidance\n",
    "            lowres_cond_img = lowres_cond_img,\n",
    "            lowres_noise_times = self.lowres_noise_schedule.get_condition(lowres_noise_times) ))\n",
    "\n",
    "        if pred_objective == 'noise':\n",
    "            x_start = noise_scheduler.predict_start_from_noise(x, t = t, noise = pred)\n",
    "        elif pred_objective == 'x_start':\n",
    "            x_start = pred\n",
    "        elif pred_objective == 'v':\n",
    "            x_start = noise_scheduler.predict_start_from_v(x, t = t, v = pred)\n",
    "        else:\n",
    "            raise ValueError(f'unknown objective {pred_objective}')\n",
    "\n",
    "        if dynamic_threshold:\n",
    "            s = torch.quantile(\n",
    "                rearrange(x_start, 'b ... -> b (...)').abs(),\n",
    "                self.dynamic_thresholding_percentile,\n",
    "                dim = -1\n",
    "            )\n",
    "\n",
    "            s.clamp_(min = 1.)\n",
    "            s = right_pad_dims_to(x_start, s)\n",
    "            x_start = x_start.clamp(-s, s) / s\n",
    "        else:\n",
    "            x_start.clamp_(-1., 1.)\n",
    "\n",
    "        mean_and_variance = noise_scheduler.q_posterior(x_start = x_start, x_t = x, t = t, t_next = t_next)\n",
    "        return mean_and_variance, x_start\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(\n",
    "        self,\n",
    "        unet,\n",
    "        x,\n",
    "        t,\n",
    "        *,\n",
    "        noise_scheduler,\n",
    "        t_next = None,\n",
    "        cond_scale = 1.,\n",
    "        lowres_cond_img = None,\n",
    "        lowres_noise_times = None,\n",
    "        pred_objective = 'noise',\n",
    "        dynamic_threshold = True\n",
    "    ):\n",
    "        b, *_, device = *x.shape, x.device\n",
    "\n",
    "\n",
    "        (model_mean, _, model_log_variance), x_start = self.p_mean_variance(\n",
    "            unet,\n",
    "            x = x,\n",
    "            t = t,\n",
    "            t_next = t_next,\n",
    "            noise_scheduler = noise_scheduler,\n",
    "            cond_scale = cond_scale,\n",
    "            lowres_cond_img = lowres_cond_img,\n",
    "            lowres_noise_times = lowres_noise_times,\n",
    "            pred_objective = pred_objective,\n",
    "            dynamic_threshold = dynamic_threshold,\n",
    "        )\n",
    "\n",
    "        noise = torch.randn_like(x)\n",
    "        # no noise when t == 0\n",
    "        is_last_sampling_timestep = (t_next == 0) if isinstance(noise_scheduler, GaussianDiffusionContinuousTimes) else (t == 0)\n",
    "        nonzero_mask = (1 - is_last_sampling_timestep.float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n",
    "        pred = model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n",
    "        return pred, x_start\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(\n",
    "        self,\n",
    "        unet,\n",
    "        shape,\n",
    "        *,\n",
    "        noise_scheduler,\n",
    "        lowres_cond_img = None,\n",
    "        lowres_noise_times = None,\n",
    "        cond_scale = 1,\n",
    "        pred_objective = 'noise',\n",
    "        dynamic_threshold = True,\n",
    "        use_tqdm = True\n",
    "        ):\n",
    "        \n",
    "        device =\"cuda\"\n",
    "        \n",
    "        batch = shape[0]\n",
    "        img = torch.randn(shape, device = device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x_start = None\n",
    "\n",
    "        # time\n",
    "        timesteps = noise_scheduler.get_sampling_timesteps(batch, device = device)\n",
    "\n",
    "\n",
    "        timesteps = timesteps[0:]\n",
    "\n",
    "        for times, times_next in tqdm(timesteps, desc = 'sampling loop time step', total = len(timesteps), disable = not use_tqdm):\n",
    "            is_last_timestep = times_next == 0\n",
    "\n",
    "            for r in reversed(range(1)):\n",
    "                is_last_resample_step = r == 0\n",
    "\n",
    "                img, x_start = self.p_sample(\n",
    "                    unet,\n",
    "                    img,\n",
    "                    times,\n",
    "                    t_next = times_next,\n",
    "                    cond_scale = cond_scale,\n",
    "                    lowres_cond_img = lowres_cond_img,\n",
    "                    lowres_noise_times = lowres_noise_times,\n",
    "                    noise_scheduler = noise_scheduler,\n",
    "                    pred_objective = pred_objective,\n",
    "                    dynamic_threshold = dynamic_threshold,\n",
    "                )\n",
    "                \n",
    "        \n",
    "        img.clamp_(-1., 1.)\n",
    "        \n",
    "        return img\n",
    "\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    @eval_decorator\n",
    "    @beartype\n",
    "    def restoration(\n",
    "        self,\n",
    "        batch_size = 1,\n",
    "        cond_scale = 1.,\n",
    "        lowres_sample_noise_level = None,\n",
    "        start_image_or_video = None,\n",
    "        device = None,\n",
    "        use_tqdm = True,\n",
    "        use_one_unet_in_gpu = True\n",
    "        ):\n",
    "\n",
    "        device = default(device, \"cuda\")\n",
    "        self.reset_unets_all_one_device(device = device)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        is_cuda = next(self.parameters()).is_cuda\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        lowres_sample_noise_level = default(lowres_sample_noise_level, self.lowres_sample_noise_level)\n",
    "\n",
    "\n",
    "        # condition scaling\n",
    "        cond_scale = cond_scale\n",
    "\n",
    "\n",
    "        assert exists(start_image_or_video), 'starting image must be supplied if only doing upscaling'\n",
    "\n",
    "        prev_image_size = self.image_sizes\n",
    "        prev_frame_size =  None\n",
    "        \n",
    "        \n",
    "        img = self.resize_to(start_image_or_video, prev_image_size)\n",
    "        \n",
    "\n",
    "        unet = self.unets\n",
    "        channel = self.sample_channels\n",
    "        image_size = self.image_sizes\n",
    "        noise_scheduler = self.noise_schedulers\n",
    "        pred_objective = self.pred_objectives\n",
    "        dynamic_threshold = self.dynamic_thresholding\n",
    "        unet_cond_scale = cond_scale\n",
    "\n",
    "        \n",
    "        context = self.one_unet_in_gpu(unet = unet) if is_cuda and use_one_unet_in_gpu else nullcontext()\n",
    "\n",
    "        with context:\n",
    "\n",
    "            # low resolution conditioning\n",
    "            lowres_cond_img = lowres_noise_times = None\n",
    "\n",
    "            shape = (batch_size, channel, image_size, image_size)\n",
    "\n",
    "            resize_kwargs = dict()\n",
    "            \n",
    "            if unet.lowres_cond:\n",
    "                #lowres_noise_times = self.lowres_noise_schedule.get_times(batch_size, lowres_sample_noise_level, device = device)\n",
    "                lowres_noise_times = self.lowres_noise_schedule.get_times(img.shape[0], lowres_sample_noise_level, device = device)\n",
    "                lowres_cond_img = self.resize_to(img, image_size, **resize_kwargs)\n",
    "                lowres_cond_img, *_ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img,\n",
    "                                                                          t = lowres_noise_times, \n",
    "                                                                          noise = torch.randn_like(lowres_cond_img))\n",
    "\n",
    "            \n",
    "            shape = (lowres_cond_img.shape[0], self.channels, image_size, image_size)\n",
    "            \n",
    "            \n",
    "            img = self.p_sample_loop(\n",
    "                unet,\n",
    "                shape,\n",
    "                cond_scale = unet_cond_scale,\n",
    "                lowres_cond_img = lowres_cond_img,\n",
    "                lowres_noise_times = lowres_noise_times,\n",
    "                noise_scheduler = noise_scheduler,\n",
    "                pred_objective = pred_objective,\n",
    "                dynamic_threshold = dynamic_threshold,\n",
    "                use_tqdm = use_tqdm,\n",
    "            )\n",
    "\n",
    "            outputs.append(img)\n",
    "        \n",
    "        return outputs[-1]\n",
    "\n",
    "\n",
    "    @beartype\n",
    "    def p_losses(\n",
    "        self,\n",
    "        unet: Union[UNet],\n",
    "        x_start,\n",
    "        times,\n",
    "        *,\n",
    "        noise_scheduler,\n",
    "        lowres_cond_img = None,\n",
    "        lowres_aug_times = None,\n",
    "        noise = None,\n",
    "        times_next = None,\n",
    "        pred_objective = 'noise',\n",
    "        min_snr_gamma = None,\n",
    "        random_crop_size = None,\n",
    "        **kwargs\n",
    "        ):\n",
    "                \n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "\n",
    "        # get x_t\n",
    "        x_noisy, log_snr, alpha, sigma = noise_scheduler.q_sample(x_start = x_start, t = times, noise = noise)\n",
    "\n",
    "        \n",
    "        # also noise the lowres conditioning image\n",
    "        # at sample time, they then fix the noise level of 0.1 - 0.3\n",
    "        lowres_cond_img_noisy = None\n",
    "        if exists(lowres_cond_img):\n",
    "            lowres_aug_times = default(lowres_aug_times, times)\n",
    "            lowres_cond_img_noisy, *_ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img,\n",
    "                                                                            t = lowres_aug_times, \n",
    "                                                                            noise = torch.randn_like(lowres_cond_img))\n",
    "\n",
    "        # time condition\n",
    "        noise_cond = noise_scheduler.get_condition(times)\n",
    "\n",
    "        # unet kwargs\n",
    "        unet_kwargs = dict(\n",
    "                            lowres_noise_times = self.lowres_noise_schedule.get_condition(lowres_aug_times),\n",
    "                            lowres_cond_img = lowres_cond_img_noisy,\n",
    "                            cond_drop_prob = self.cond_drop_prob,\n",
    "                            **kwargs)\n",
    "\n",
    "        # get prediction\n",
    "        pred = unet.forward(\n",
    "                            x_noisy,\n",
    "                            noise_cond,\n",
    "                            **unet_kwargs)\n",
    "\n",
    "        # prediction objective\n",
    "        if pred_objective == 'noise':\n",
    "            target = noise\n",
    "        elif pred_objective == 'x_start':\n",
    "            target = x_start\n",
    "        elif pred_objective == 'v':\n",
    "            # derivation detailed in Appendix D of Progressive Distillation paper\n",
    "            # https://arxiv.org/abs/2202.00512\n",
    "            # this makes distillation viable as well as solve an issue with color shifting in upresoluting unets, noted in imagen-video\n",
    "            target = alpha * noise - sigma * x_start\n",
    "        else:\n",
    "            raise ValueError(f'unknown objective {pred_objective}')\n",
    "\n",
    "        # losses\n",
    "        losses = self.loss_fn(pred, target, reduction = 'none')\n",
    "        losses = reduce(losses, 'b ... -> b', 'mean')\n",
    "\n",
    "        # min snr loss reweighting\n",
    "        snr = log_snr.exp()\n",
    "        maybe_clipped_snr = snr.clone()\n",
    "\n",
    "        if exists(min_snr_gamma):\n",
    "            maybe_clipped_snr.clamp_(max = min_snr_gamma)\n",
    "\n",
    "        if pred_objective == 'noise':\n",
    "            loss_weight = maybe_clipped_snr / snr\n",
    "        elif pred_objective == 'x_start':\n",
    "            loss_weight = maybe_clipped_snr\n",
    "        elif pred_objective == 'v':\n",
    "            loss_weight = maybe_clipped_snr / (snr + 1)\n",
    "\n",
    "        losses = losses * loss_weight\n",
    "        return losses.mean()\n",
    "\n",
    "\n",
    "    @beartype\n",
    "    def forward(self\n",
    "                ,images\n",
    "                ,lowres_cond_img\n",
    "                ,**kwargs):\n",
    "        \n",
    "        unet = self.unets\n",
    "\n",
    "        images = cast_uint8_images_to_float(images)\n",
    "\n",
    "        assert images.dtype == torch.float or images.dtype == torch.half, f'images tensor needs to be floats but {images.dtype} dtype found instead'\n",
    "        \n",
    "        b, c, *_, h, w, device = *images.shape, images.device\n",
    "        \n",
    "        assert images.shape[1] == self.channels\n",
    "        assert h >= self.image_sizes and w >= self.image_sizes\n",
    "\n",
    "        times = self.noise_schedulers.sample_random_times(b, device = device)\n",
    "\n",
    "        \n",
    "        #prev_image_size = 64\n",
    "        #lowres_cond_img = self.resize_to(images, prev_image_size,  clamp_range = [-1,1] )\n",
    "        #lowres_cond_img = self.resize_to(lowres_cond_img, self.image_sizes, clamp_range = [-1,1] )\n",
    "\n",
    "        lowres_aug_time = self.lowres_noise_schedule.sample_random_times(1, device = device)\n",
    "        lowres_aug_times = repeat(lowres_aug_time, '1 -> b', b = b)\n",
    "        \n",
    "\n",
    "        images = self.resize_to(images, self.image_sizes)\n",
    "        \n",
    "        return self.p_losses(unet, images, times, \n",
    "                             noise_scheduler = self.noise_schedulers, lowres_cond_img = lowres_cond_img, \n",
    "                             lowres_aug_times = lowres_aug_times, pred_objective = self.pred_objectives, \n",
    "                             min_snr_gamma = self.min_snr_gamma, **kwargs)\n",
    "\n",
    "\n",
    "class Palette(BaseModel):\n",
    "    def __init__(self, networks, losses, sample_num, task, optimizers, log_iter,\n",
    "                 model_path, dataloader_circ_1, dataloader_circ_2, ema_scheduler=None,scale=1,  **kwargs):\n",
    "        ''' must to init BaseModel with kwargs '''\n",
    "        super(Palette, self).__init__(**kwargs)\n",
    "\n",
    "        ''' networks, dataloder, optimizers, losses, etc. '''\n",
    "        self.model_path = model_path\n",
    "        self.log_iter = log_iter\n",
    "        self.loss_fn = losses\n",
    "        self.netG = networks\n",
    "        self.dataloader_circ_1 = dataloader_circ_1\n",
    "        self.dataloader_circ_2 = dataloader_circ_2\n",
    "\n",
    "        if ema_scheduler is not None:\n",
    "            self.ema_scheduler = ema_scheduler\n",
    "            self.netG_EMA = copy.deepcopy(self.netG)\n",
    "            self.EMA = EMA(beta=self.ema_scheduler['ema_decay'])\n",
    "        else:\n",
    "            self.ema_scheduler = None\n",
    "        \n",
    "        ''' networks can be a list, and must convert by self.set_device function if using multiple GPU. '''\n",
    "        self.netG.to(self.device)\n",
    "        if self.ema_scheduler is not None:\n",
    "            self.netG_EMA = self.netG.to(self.device) \n",
    "        self.load_networks(self.model_path)\n",
    "\n",
    "        self.optG = torch.optim.Adam(list(filter(lambda p: p.requires_grad, self.netG.parameters())), **optimizers)\n",
    "        self.optimizers.append(self.optG)\n",
    "        self.resume_training() \n",
    "\n",
    "        #self.netG.set_loss(self.loss_fn)\n",
    "\n",
    "        self.sample_num = sample_num\n",
    "        self.task = task\n",
    "        self.scale=scale\n",
    "        \n",
    "    def set_input(self, data):\n",
    "        ''' must use set_device in tensor '''\n",
    "        self.cond_image = data.get('cond_image').to(self.device)\n",
    "        self.gt_image = data.get('gt_image').to(self.device)\n",
    "    \n",
    "        self.path = data['path']\n",
    "        self.batch_size = len(data['path'])\n",
    "    \n",
    "    def get_current_visuals(self, phase='train'):\n",
    "        dict = {\n",
    "            'gt_image': (self.gt_image.detach()[:].float().cpu()+1)/2,\n",
    "            'cond_image': (self.cond_image.detach()[:].float().cpu()+1)/2,\n",
    "        }\n",
    "\n",
    "        if phase != 'train':\n",
    "            dict.update({\n",
    "                'output': (self.output.detach()[:].float().cpu()+1)/2\n",
    "            })\n",
    "        return dict\n",
    "\n",
    "    def save_current_results(self):\n",
    "        ret_path = []\n",
    "        ret_result = []\n",
    "        for idx in range(self.batch_size):\n",
    "            ret_path.append('GT_{}'.format(self.path[idx]))\n",
    "            ret_result.append(self.gt_image[idx].detach().float().cpu())\n",
    "\n",
    "            ret_path.append('Process_{}'.format(self.path[idx]))\n",
    "            ret_result.append(self.visuals[idx::self.batch_size].detach().float().cpu())\n",
    "            \n",
    "            ret_path.append('Out_{}'.format(self.path[idx]))\n",
    "            ret_result.append(self.visuals[idx-self.batch_size].detach().float().cpu())\n",
    "        \n",
    "\n",
    "        self.results_dict = self.results_dict._replace(name=ret_path, result=ret_result)\n",
    "        return self.results_dict._asdict()\n",
    "\n",
    "\n",
    "    def train_step(self):\n",
    "        self.netG.train()\n",
    "        total_loss = 0.0  \n",
    "        \n",
    "        total_batches = len(self.phase_loader.dataset) // self.phase_loader.batch_size\n",
    "        pbar = tqdm(total=total_batches, position=0, leave=True)\n",
    "        for i, elements in enumerate(zip(self.phase_loader, self.dataloader_circ_1)):\n",
    "        \n",
    "            self.optG.zero_grad()\n",
    "            self.gt_image, self.cond_image_1 = elements\n",
    "            self.gt_image = self.gt_image.float().to(self.device)\n",
    "            self.cond_image = self.cond_image_1.float().to(self.device)\n",
    "            \n",
    "            self.cond_image,_ ,_ ,_  = imagen.noise_schedulers.q_sample(self.cond_image.to(\"cpu\"),torch.tensor(50))\n",
    "            self.cond_image = torch.clip(self.cond_image,-1,1).to(self.device)\n",
    "            \n",
    "\n",
    "            loss  = self.netG(self.gt_image, self.cond_image)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            self.optG.step()\n",
    "\n",
    "            # Accumulate total loss for the epoch\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            self.iter += self.batch_size\n",
    "\n",
    "            if self.ema_scheduler is not None:\n",
    "                if self.iter > self.ema_scheduler['ema_start'] and self.iter % self.ema_scheduler['ema_iter'] == 0:\n",
    "                    self.EMA.update_model_average(self.netG_EMA, self.netG)\n",
    "\n",
    "            for scheduler in self.schedulers:\n",
    "                scheduler.step()\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "\n",
    "        avg_loss = total_loss / len(self.phase_loader)\n",
    "        print(f\"Avg Loss for Epoch: {avg_loss}\")\n",
    "        wandb.log({\"loss\": avg_loss})\n",
    "\n",
    "        return avg_loss, self.cond_image, self.gt_image\n",
    "    \n",
    "    \n",
    "\n",
    "    def test(self, use_tqdm=False):\n",
    "        self.netG.eval()\n",
    "        with torch.no_grad():\n",
    "            total_batches = len(self.phase_loader.dataset) // self.phase_loader.batch_size\n",
    "            pbar = tqdm(total=total_batches, position=0, leave=True)\n",
    "            for i, elements in enumerate(zip(self.phase_loader, self.dataloader_circ_2)):\n",
    "                if i == 0:\n",
    "                    self.gt_image, self.cond_image_1  = elements\n",
    "                    break \n",
    "                \n",
    "            self.gt_image = self.gt_image.float().to(self.device)\n",
    "            self.cond_image_ = self.cond_image_1.unsqueeze(1).float().to(self.device)\n",
    "            \n",
    "            self.cond_image,_ ,_ ,_  = imagen.noise_schedulers.q_sample(self.cond_image_.to(\"cpu\"),torch.tensor(50))\n",
    "            self.cond_image = torch.clip(self.cond_image,-1,1).to(self.device)\n",
    "            \n",
    "            self.output = self.netG.restoration(start_image_or_video=self.cond_image, use_tqdm=use_tqdm)\n",
    "            \n",
    "            \n",
    "            \n",
    "            print(\"diffusion generated sample\")\n",
    "            plot_images_no_lab(self.output[:8])\n",
    "            print(\"condition sample\")\n",
    "            plot_images_no_lab(self.cond_image[:8])\n",
    "            print(\"original sample\")\n",
    "            plot_images_no_lab(self.gt_image[:8])\n",
    "\n",
    "\n",
    "            print(\"BLUE: ORIGINAL DATA VS ORANGE: DIFFUSION MODEL GENERATED\")\n",
    "            latitudinal_mean_three(self.gt_image.detach(),\n",
    "                             self.output.detach(),\n",
    "                             self.cond_image_.detach()\n",
    "                            ,label_name=[\"hr era5\",\"sr gfdl\",\"lr gfdl\"]) \n",
    "\n",
    "            print(\"BLUE: ORIGINAL DATA VS ORANGE: DIFFUSION MODEL GENERATED, GREEN: UNET GENERATED\")\n",
    "            histograms_three(self.gt_image.detach(),\n",
    "                             self.output.detach(),\n",
    "                             self.cond_image_.detach()\n",
    "                             ,xlim_end=None, var=\"p\"\n",
    "                             ,label_name=[\"hr era5\",\"sr gfdl\",\"lr gfdl\"])\n",
    "            \n",
    "            ssd = SpatialSpectralDensity_diff_res( \n",
    "                                     self.gt_image.detach().cpu().numpy()\n",
    "                                     ,self.output.detach().cpu().numpy()\n",
    "                                     ,self.cond_image_.detach().cpu().numpy()\n",
    "                                     ,new_labels = [\"hr era5\",\"sr gfdl\",\"lr gfdl\"])\n",
    "            ssd.run(num_times=None)\n",
    "            ssd.plot_psd(fname=f'/dss/dsshome1/0D/ge74xuf2/climate_diffusion/results/psd/psd_gfdl_era5_lr_vs_hr_vanilla.pdf'\n",
    "                         ,model_resolution=0.25,model_resolution_2=0.25)\n",
    "\n",
    "        return self.output\n",
    "\n",
    "\n",
    "    def load_networks(self, model_path):\n",
    "        \"\"\" save pretrained model and training state, which only do on GPU 0. \"\"\"\n",
    "        netG_label = self.netG.__class__.__name__\n",
    "        self.load_network(network=self.netG, network_label=netG_label, model_path=model_path, strict=False)\n",
    "        if self.ema_scheduler is not None:\n",
    "            self.load_network(network=self.netG_EMA, network_label=netG_label+'_ema',model_path=model_path, strict=False)\n",
    "          \n",
    "        \n",
    "    def load_pretrain_diffusion(self, model_path):\n",
    "        self.netG.load_state_dict(torch.load(model_path), strict=False)\n",
    "        self.netG.to(self.device)\n",
    "        \n",
    "        if self.ema_scheduler is not None:\n",
    "            self.netG_EMA.load_state_dict(torch.load(model_path), strict=False)\n",
    "            self.netG_EMA.to(self.device)\n",
    "            return self.netG_EMA\n",
    "        return self.netG\n",
    "        \n",
    "    \n",
    "    \n",
    "    def save_everything(self):\n",
    "        \"\"\" load pretrained model and training state. \"\"\"\n",
    "        netG_label = self.netG.__class__.__name__\n",
    "        self.save_network(network=self.netG, network_label=netG_label)\n",
    "        if self.ema_scheduler is not None:\n",
    "            self.save_network(network=self.netG_EMA, network_label=netG_label+'_ema')\n",
    "        self.save_training_state()\n",
    "\n",
    "CustomResult = collections.namedtuple('CustomResult', 'name result')\n",
    "\n",
    "\n",
    "def mse_loss(output, target):\n",
    "    return F.mse_loss(output, target)\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# +\n",
    "kwargs = {\n",
    "    \"phase\": \"train\",\n",
    "    \"dataloader\": era5_hr_dl, \n",
    "    \"metrics\": [\"mae\"],\n",
    "    \"resume_state\" : \"/dss/dssfs04/lwp-dss-0002/pn49fu/pn49fu-dss-0002/ge74xuf2/diffusion/revision_100/100\",  #\"\", # \"/dss/dssfs04/lwp-dss-0002/pn49fu/pn49fu-dss-0002/ge74xuf2/diffusion/bc_sr_once_imagen/41\", # \"\", #  \n",
    "    \"n_epochs\" : 400, \n",
    "    \"batch_size\" : config[\"batch_size\"],\n",
    "    \"save_checkpoint_epoch\" : 10, \n",
    "    \"save_path_base\":\"/dss/dssfs04/lwp-dss-0002/pn49fu/pn49fu-dss-0002/ge74xuf2/diffusion\",\n",
    "}\n",
    "\n",
    "\n",
    "imagen_unet = SRUnet256(num_resnet_blocks = (2, 4, 8, 8))\n",
    "\n",
    "imagen = Imagen(\n",
    "            unets = imagen_unet,\n",
    "            image_sizes = 256,\n",
    "            timesteps = 100,      \n",
    "            cond_drop_prob = 0.1, # is trained with 0 (no clf - g)\n",
    "            )\n",
    "\n",
    "\n",
    "palette_model = Palette(\n",
    "    networks=imagen,\n",
    "    losses=mse_loss,\n",
    "    sample_num=8,\n",
    "    task=\"inpainting\",\n",
    "    optimizers={\"lr\": 1e-4, \"weight_decay\": 0},  # was 5e-5\n",
    "    log_iter = 1000,                                                            \n",
    "    model_path = \"/dss/dssfs04/lwp-dss-0002/pn49fu/pn49fu-dss-0002/ge74xuf2/diffusion/revision_100/100_Imagen.pth\",    #    \"\", #  \n",
    "    dataloader_circ_1 = era5_lr_dl,  \n",
    "    dataloader_circ_2 = era5_lr_dl, #dataloader_bc_gfdl,\n",
    "    scale=0.5,\n",
    "    ema_scheduler = None,\n",
    "    #{\"ema_start\": 1,\n",
    "    #\"ema_iter\": 1,\n",
    "    #\"ema_decay\": 0.9999},\n",
    "    **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b629bc93-0228-465b-adf3-9e3b8bd46f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_training = False\n",
    "\n",
    "\n",
    "if do_training==True:\n",
    "    palette_model_result = palette_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b077de-cf98-4421-84ec-93c70df76dac",
   "metadata": {},
   "source": [
    "# create full DM corrected 1992-2014 historical dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4374a67-21d7-4305-878e-9064508c9916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataloader_sr import gfdl_eval, GFDL_P_Dataset_64_ssp_1995_2014_original_unit_after_QM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a21d375d-50d4-4ef9-aef9-8c3752fe38a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/0D/ge74xuf2/climate_diffusion/src/dataloader_sr.py:161: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.era5 = torch.load(self.era5_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFDL QM ssp 64 shape: torch.Size([10, 1, 64, 64])\n",
      "GFDL 1992-2014 64 shape: torch.Size([10, 1, 64, 64])\n",
      "HR ERA5 torch.Size([10, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "do_eval_sr_dataset = True\n",
    "\n",
    "bs_valid = 10\n",
    "\n",
    "if do_eval_sr_dataset == True:\n",
    "    \n",
    "    gfdl_hist_qm_ds = GFDL_P_Dataset_64_ssp_1995_2014_original_unit_after_QM()\n",
    "    gfdl_hist_qm_dl = data.DataLoader(gfdl_hist_qm_ds, batch_size=bs_valid, shuffle=False,drop_last=True)\n",
    "\n",
    "    gfdl_hist_qm = next(iter(gfdl_hist_qm_dl))\n",
    "    print(\"GFDL QM ssp 64 shape:\",gfdl_hist_qm.shape)\n",
    "\n",
    "if do_eval_sr_dataset == True:\n",
    "    \n",
    "    gfdl_hist_ds = gfdl_eval(\"train\")\n",
    "    gfdl_hist_dl = data.DataLoader(gfdl_hist_ds, batch_size=bs_valid, shuffle=False,drop_last=True)\n",
    "\n",
    "    gfdl_hist = next(iter(gfdl_hist_dl))\n",
    "    print(\"GFDL 1992-2014 64 shape:\",gfdl_hist.shape)\n",
    "\n",
    "if do_eval_sr_dataset == True:\n",
    "    era5_hr_ds = era5_0_25d_256(stage='train')\n",
    "    era5_hr_dl = data.DataLoader(era5_hr_ds, batch_size=bs_valid, shuffle=False, drop_last=True)\n",
    "    era5_hr_val = next(iter(era5_hr_dl))\n",
    "    print(\"HR ERA5\", era5_hr_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5febda2-1b5f-4802-865b-ccd3438df028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7300, 6936, 6941)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gfdl_hist_qm_ds), len(gfdl_hist_ds), len(era5_hr_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "479bc180-5d1c-4db3-ae55-53e15ebe20b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.7021), tensor(0.4947, dtype=torch.float64))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gfdl_hist.max(), gfdl_hist_qm_ds.fw_trafo(gfdl_hist_qm).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5dcddd-154f-4340-9e36-b9a923343431",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1619518/2110193946.py:983: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.netG.load_state_dict(torch.load(model_path), strict=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 0\n",
      "Processing batch 1\n",
      "Processing batch 2\n",
      "Processing batch 3\n",
      "Processing batch 4\n",
      "Processing batch 5\n",
      "Processing batch 6\n",
      "Processing batch 7\n",
      "Processing batch 8\n",
      "Processing batch 9\n",
      "Processing batch 10\n",
      "Processing batch 11\n",
      "Processing batch 12\n",
      "Processing batch 13\n",
      "Processing batch 14\n",
      "Processing batch 15\n",
      "Processing batch 16\n",
      "Processing batch 17\n",
      "Processing batch 18\n",
      "Processing batch 19\n",
      "Processing batch 20\n",
      "Processing batch 21\n",
      "Processing batch 22\n",
      "Processing batch 23\n",
      "Processing batch 24\n",
      "Processing batch 25\n",
      "Processing batch 26\n",
      "Processing batch 27\n",
      "Processing batch 28\n",
      "Processing batch 29\n",
      "Processing batch 30\n",
      "Processing batch 31\n",
      "Processing batch 32\n",
      "Processing batch 33\n",
      "Processing batch 34\n",
      "Processing batch 35\n",
      "Processing batch 36\n",
      "Processing batch 37\n",
      "Processing batch 38\n",
      "Processing batch 39\n",
      "Processing batch 40\n",
      "Processing batch 41\n",
      "Processing batch 42\n",
      "Processing batch 43\n",
      "Processing batch 44\n",
      "Processing batch 45\n",
      "Processing batch 46\n",
      "Processing batch 47\n",
      "Processing batch 48\n",
      "Processing batch 49\n",
      "Processing batch 50\n",
      "Processing batch 51\n",
      "Processing batch 52\n",
      "Processing batch 53\n",
      "Processing batch 54\n",
      "Processing batch 55\n",
      "Processing batch 56\n",
      "Processing batch 57\n",
      "Processing batch 58\n",
      "Processing batch 59\n",
      "Processing batch 60\n",
      "Processing batch 61\n",
      "Processing batch 62\n",
      "Processing batch 63\n",
      "Processing batch 64\n",
      "Processing batch 65\n",
      "Processing batch 66\n",
      "Processing batch 67\n",
      "Processing batch 68\n",
      "Processing batch 69\n",
      "Processing batch 70\n",
      "Processing batch 71\n",
      "Processing batch 72\n",
      "Processing batch 73\n",
      "Processing batch 74\n",
      "Processing batch 75\n",
      "Processing batch 76\n",
      "Processing batch 77\n",
      "Processing batch 78\n",
      "Processing batch 79\n",
      "Processing batch 80\n",
      "Processing batch 81\n",
      "Processing batch 82\n",
      "Processing batch 83\n",
      "Processing batch 84\n",
      "Processing batch 85\n",
      "Processing batch 86\n",
      "Processing batch 87\n",
      "Processing batch 88\n",
      "Processing batch 89\n",
      "Processing batch 90\n",
      "Processing batch 91\n",
      "Processing batch 92\n",
      "Processing batch 93\n",
      "Processing batch 94\n",
      "Processing batch 95\n",
      "Processing batch 96\n",
      "Processing batch 97\n",
      "Processing batch 98\n",
      "Processing batch 99\n",
      "Processing batch 100\n",
      "Processing batch 101\n",
      "Processing batch 102\n",
      "Processing batch 103\n",
      "Processing batch 104\n",
      "Processing batch 105\n",
      "Processing batch 106\n",
      "Processing batch 107\n",
      "Processing batch 108\n",
      "Processing batch 109\n",
      "Processing batch 110\n",
      "Processing batch 111\n",
      "Processing batch 112\n",
      "Processing batch 113\n",
      "Processing batch 114\n",
      "Processing batch 115\n",
      "Processing batch 116\n",
      "Processing batch 117\n",
      "Processing batch 118\n",
      "Processing batch 119\n",
      "Processing batch 120\n",
      "Processing batch 121\n",
      "Processing batch 122\n",
      "Processing batch 123\n",
      "Processing batch 124\n",
      "Processing batch 125\n",
      "Processing batch 126\n",
      "Processing batch 127\n",
      "Processing batch 128\n",
      "Processing batch 129\n",
      "Processing batch 130\n",
      "Processing batch 131\n",
      "Processing batch 132\n",
      "Processing batch 133\n",
      "Processing batch 134\n",
      "Processing batch 135\n",
      "Processing batch 136\n",
      "Processing batch 137\n",
      "Processing batch 138\n",
      "Processing batch 139\n",
      "Processing batch 140\n",
      "Processing batch 141\n",
      "Processing batch 142\n",
      "Processing batch 143\n",
      "Processing batch 144\n",
      "Processing batch 145\n",
      "Processing batch 146\n",
      "Processing batch 147\n",
      "Processing batch 148\n",
      "Processing batch 149\n",
      "Processing batch 150\n",
      "Processing batch 151\n",
      "Processing batch 152\n",
      "Processing batch 153\n",
      "Processing batch 154\n",
      "Processing batch 155\n",
      "Processing batch 156\n",
      "Processing batch 157\n",
      "Processing batch 158\n",
      "Processing batch 159\n",
      "Processing batch 160\n",
      "Processing batch 161\n",
      "Processing batch 162\n",
      "Processing batch 163\n",
      "Processing batch 164\n",
      "Processing batch 165\n",
      "Processing batch 166\n",
      "Processing batch 167\n",
      "Processing batch 168\n",
      "Processing batch 169\n",
      "Processing batch 170\n",
      "Processing batch 171\n",
      "Processing batch 172\n",
      "Processing batch 173\n",
      "Processing batch 174\n",
      "Processing batch 175\n",
      "Processing batch 176\n",
      "Processing batch 177\n",
      "Processing batch 178\n",
      "Processing batch 179\n",
      "Processing batch 180\n",
      "Processing batch 181\n",
      "Processing batch 182\n",
      "Processing batch 183\n",
      "Processing batch 184\n",
      "Processing batch 185\n",
      "Processing batch 186\n",
      "Processing batch 187\n",
      "Processing batch 188\n",
      "Processing batch 189\n",
      "Processing batch 190\n",
      "Processing batch 191\n",
      "Processing batch 192\n",
      "Processing batch 193\n",
      "Processing batch 194\n",
      "Processing batch 195\n",
      "Processing batch 196\n",
      "Processing batch 197\n",
      "Processing batch 198\n"
     ]
    }
   ],
   "source": [
    "do_save_sr_dataset_gfdl = True\n",
    "\n",
    "\n",
    "years_per_chunk = 10\n",
    "save_base_path = '/dss/dssfs04/lwp-dss-0002/pn49fu/pn49fu-dss-0002/ge74xuf2/diffusion/revision_dm_gfdl_1995_2014/DM_qm_gfdl_s100_e100_'\n",
    "\n",
    "# Load trained model\n",
    "if do_save_sr_dataset_gfdl:\n",
    "    m_path=\"/dss/dssfs04/lwp-dss-0002/pn49fu/pn49fu-dss-0002/ge74xuf2/diffusion/revision_100/100_Imagen.pth\"   \n",
    "    dm_loaded = palette_model.load_pretrain_diffusion(m_path)\n",
    "    \n",
    "    output_tensors = []\n",
    "    total_processed = 0  # Track total number of days processed\n",
    "    year_chunk_start = 1  # For naming the saved chunks\n",
    "    days_per_chunk = years_per_chunk * 365  # 10 years' worth of daily data\n",
    "    \n",
    "    for b, el in enumerate(gfdl_hist_qm_dl):\n",
    "        batch_size = el.shape[0]  # The number of samples in the current batch\n",
    "        # Upsample and process the data\n",
    "        print(f\"Processing batch {b}\")\n",
    "        el = gfdl_hist_qm_ds.fw_trafo(el)\n",
    "        el = el.to(\"cuda\").float()\n",
    "        gfdl_qm,_ ,_ ,_  = imagen.noise_schedulers.q_sample(el.to(\"cpu\"),torch.tensor(50))\n",
    "        gfdl_qm = torch.clip(gfdl_qm,-1,1)\n",
    "        \n",
    "        sr_era5_output = dm_loaded.restoration(start_image_or_video=gfdl_qm.to(\"cuda\").float(), use_tqdm=False)\n",
    "\n",
    "\n",
    "        #plot_images_no_lab(el[:7])\n",
    "        #plot_images_no_lab(sr_era5_output[:7])\n",
    "\n",
    "        \n",
    "        # Append processed output to the list\n",
    "        output_tensors.append(sr_era5_output)\n",
    "        total_processed += batch_size  # Increment total processed days by the batch size\n",
    "        \n",
    "        # Check if enough data has been processed for the chunk (10 years)\n",
    "        if total_processed >= days_per_chunk:\n",
    "            # Save the dataset for this chunk\n",
    "            save_path = f'{save_base_path}{year_chunk_start}-{year_chunk_start + years_per_chunk - 1}y.pth'\n",
    "            torch.save(torch.cat(output_tensors, dim=0).cpu(), save_path)\n",
    "            print(f\"Saving to: {save_path}\")\n",
    "\n",
    "            # Reset for next chunk\n",
    "            output_tensors = []\n",
    "            total_processed = 0\n",
    "            year_chunk_start += years_per_chunk  # Move to the next 10-year period\n",
    "    \n",
    "    # Save any remaining data that wasn't saved in the last chunk\n",
    "    if output_tensors:\n",
    "        save_path = f'{save_base_path}{year_chunk_start}-{year_chunk_start + years_per_chunk - 1}y.pth'\n",
    "        torch.save(torch.cat(output_tensors, dim=0).cpu(), save_path)\n",
    "        print(f\"Saving to: {save_path}\")\n",
    "\n",
    "    print(\"Processing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22034392-4c13-4da3-914d-42d6c002cc91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
